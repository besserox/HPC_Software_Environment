{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HPC Software Environment","text":""},{"location":"#list-of-practicals","title":"List of Practicals","text":"<ul> <li>Tuesday 20/02: Command Line Environment on HPC</li> <li>Tuesday 27/02: Compilation on HPC</li> <li>Tuesday 05/03: Debugging &amp; Profiling</li> <li>Tuesday 12/03: Software Installation on HPC</li> <li>Tuesday 19/03: Testing on HPC with ReFrame</li> <li>Tuesday 26/03: Checkpoint-Restart on HPC</li> <li>Tuesday 16/04: Containers for HPC</li> <li>Tuesday 23/04: Workflow on HPC with SnakeMake</li> <li>Tuesday 14/05: Visualization</li> </ul>"},{"location":"command_line_hpc/","title":"Command Line Environment on HPC","text":"<ul> <li>Author: Xavier Besseron (University of Luxembourg)</li> <li>License: \u00a92024 CC BY-NC-SA 4.0</li> <li>Date of practical: Tuesday 20th of February 2024</li> </ul>"},{"location":"command_line_hpc/#introduction","title":"\ud83d\udd35 Introduction","text":"<p>This course on HPC Software Environment aims to provide you with a comprehensive understanding of the software environment used in High-Performance Computing (HPC) from the perspective of application developers and users. It covers aspects related to application development, software deployment and execution of HPC workflows.</p> <p>During the semester, you will have the opportunity the learn about and get familiar with numerous tools that will make you an HPC power user. The practicals proposed over the semester will be carried out on the Uni.lu HPC platform and make intensive use of the command line and Git.</p> <p>Important</p> <p>The content of this practical is considered a technical requirement for the rest of this course. If have difficulties carrying out the exercises, it is strongly advised that you check the \"Additional Resources\" and study them carefully.</p>"},{"location":"command_line_hpc/#objectives","title":"Objectives","text":"<p>The objective of this practical is to ensure that you master the technical requirements in preparation for the rest of the semester:</p> <ul> <li>Connect to the Uni.lu HPC platform</li> <li>Use the command line interface</li> <li>Get compute nodes and submit jobs</li> <li>Work with Git</li> <li>Transfer files between your personal computer and the HPC</li> </ul> <p>The exercises will also refer to external documentation and instructions that need to be consulted.  The aim is for you to be independent and work in 'real conditions' by finding the information you need to complete the task.</p> <p></p>"},{"location":"command_line_hpc/#instructions","title":"Instructions","text":"<p>This practical is an individual work to be carried out on the Aion cluster of the HPC platform of the University of Luxembourg.</p> <p>It is composed of different elements:</p> <ul> <li>this page that contains step-by-step instructions and questions;</li> <li>the GitHub Classroom repository that contains the materials for the exercises;</li> <li>the report that you have to submit via GitHub Classroom with the rest of the requested documents</li> </ul> <p>Important: Uni.lu HPC Account</p> <p>It is assumed that you have an account on the Uni.lu HPC and that you know already how to access the platform.</p> <p>If needed, How to Get a New User account?</p> <p>Important: GitHub Account</p> <p>This practical and the following ones will rely on GitHub Classroom to download the materials and submit your results and thus it requires a GitHub account. If you don't have a GitHub account, you can sign up here.</p>"},{"location":"command_line_hpc/#report","title":"Report","text":"<p>Together with this tutorial, you need to prepare a short report of your work:</p> <ul> <li>There is no template, you can start from a blank document. Keep it simple, clear and well-organized.</li> <li>Make sure you answer all the questions (cf below), with text, code or screenshots.</li> <li>Prepare your report at the same time you're doing the exercises.</li> <li>Submit your report as a PDF file to your GitHub Classroom repository and push it before the deadline.</li> </ul> <p>Question 1</p> <p>Indicate at the beginning of your report:</p> <ul> <li>the title of the practical</li> <li>your full name</li> <li>your Uni.lu HPC username</li> <li>your GitHub username</li> <li>the date</li> </ul>"},{"location":"command_line_hpc/#part-1-accessing-the-hpc-platform","title":"\ud83d\udd35 Part 1: Accessing the HPC platform","text":"<p>In this part, the goal is to connect to the HPC platform from your personal computer. This can be a bit complicated the first time but it should become pretty smooth once everything is properly configured.</p> <p>It is assumed that you previously used the HPC of the University so this should be a problem.</p>"},{"location":"command_line_hpc/#setup-of-ssh-access","title":"Setup of SSH Access","text":"<p>Getting access to the Uni.lu HPC usually requires the following steps that have to be done once for all:</p> <ul> <li>Getting an account</li> <li>Installing an SSH client</li> <li>Creating an SSH key</li> <li>Uploading your public SSH key</li> <li>Setting up your SSH configuration</li> </ul> <p>Tip</p> <p>Working on Linux or MacOS is a great advantage for this as most of the tools are already available and follow the standard configuration. </p> <p>If you work on Windows, it is recommended to install Windows Subsystem for Linux (WSL)  and set up an Ubuntu subsystem from the Microsoft Store.  Thus, you can directly work on Linux within Windows.</p> <p>Warning</p> <p>This configuration is important as you will make use of it throughout the semester.  Without it, it will be impossible to carry out the work of this course.</p> Guidelines to Handle Your SSH Keys <p>Handling SSH Keys can be confusing and intimidating at first.  It is about security and it should be taken seriously when all the devices are connected to the Internet. Here are a few guidelines:</p> <ul> <li>The term \"pair of SSH keys\" refers to the public and private keys associated together. </li> <li>The private key is meant to be private: Never share it with anyone! Don't send it by email! (not even to yourself) Don't put it on a Cloud drive (Dropbox, Google Drive, MS OneDrive, etc.). </li> <li>Generate one pair of keys for each of your account@machine. Don't try to copy your keys around. It makes it easier to block an access if it gets compromised.</li> <li>Protect your SSH key with a passphrase. Without that, if your laptop gets stolen, your accesses to remote machines get compromised too! Instead, learn to use SSH Agent so you need to type your passphrase only once after booting your laptop.</li> <li>You're not allowed to share your HPC access with another person. If you need to do it for some reason (e.g. debugging issues specific to your account), you should NOT share your private SSH key. Instead, you should authorize the SSH public of the other person to access your account (via IPA or <code>.ssh/authorized_keys</code>)</li> </ul>"},{"location":"command_line_hpc/#connection-to-hpc-access","title":"Connection to HPC Access","text":"<p>When using a supercomputer, you will usually first connect to a front-end or access node.  From this machine, you can check your files, disk quota and computing usage.  It is intended to be used by the user to prepare his computing job and scripts and then submit them to the job scheduler.</p> <p>Because the access node is shared by all the users of the platform,  it should not be used to compile and install your software and it should definitely not be used to run any memory or computing-intensive task.</p> <p>This is an excerpt of the Acceptable Use Policy of Uni.lu HPC that you accepted to get an account on the platform:</p> <p>The frontend (access) nodes of the UL HPC clusters are reserved for data transfer to/from the clusters, preparation of job submissions to the queueing system and checking the status of computational jobs. The frontend nodes must not be used for compute or I/O bound processes. Processes inappropriate to execute on the access nodes may be killed and the user notified.</p> <p>Usually, there are some guardrails implemented on the access mode to prevent unwanted usage. For example, there is no compiler installed or the <code>module</code> command is not available.</p> <p>If you configure your access correctly, connecting to the access node of the Aion cluster should be straightforward.</p> <pre><code>ssh aion-cluster\n</code></pre> <p>Alternatively, you can connect to the Iris cluster with this command:</p> <pre><code>ssh iris-cluster\n</code></pre> <p>Info</p> <p>In the example above, <code>aion-cluster</code> does not name a specific machine.  Instead, it refers to an entry in the SSH config file that defines a set of configuration items, like the hostname (actual address on the internet),  the port of the SSH service and your username, so you don't need to remember them and enter them every time. </p> <p>If the connection is successful, you should see something like this:</p> <p></p> <p>The shell prompt (last line on the terminal) should indicate that you're correctly connected to the Aion cluster.</p> Failing to connect? <p>If you fail to connect, don't panic! Instead look for clues that would indicate the reason of the failure:</p> <ol> <li>Any error message? Check carefully all the output lines on your terminal, and try to address the first one.</li> <li>Need more information from SSH? Add the <code>-v</code> option (like verbose) after the <code>ssh</code> command. That will give you more details about the connect process. You can add more <code>v</code> to get more detailed messages.</li> <li>Call me ! \ud83d\udc68\u200d\ud83c\udfeb This time I will help you getting this right, next time you're on your own \ud83d\ude42</li> </ol> <p>You made it! You're connected to the HPC of the University of Luxembourg.</p>"},{"location":"command_line_hpc/#part-2-exploring-the-command-line-and-the-shell","title":"\ud83d\udd35 Part 2: Exploring the command line and the shell","text":"<p>It is time to explore your user account. Try these different commands:</p> <ul> <li>Who are you? <code>whoami</code>, <code>echo $USER</code>, <code>id</code></li> <li>Who is connected? <code>who</code></li> <li>What time is it? <code>date</code></li> <li>On what computer are you? <code>hostname</code>, <code>uname -a</code>, <code>lsb_release -a</code></li> <li>Where are you on the computer? <code>pwd</code></li> </ul> <p>For the commands you don't know, take the time to check what they do. For example, <code>whatis who</code>, <code>who --help</code> or <code>man who</code>.</p> <p>Question 2</p> <p>Enter commands to show </p> <ol> <li>your username</li> <li>the machine on which you're connected</li> <li>the date</li> <li>the current working directory</li> </ol> <p>Take a screenshot and add it to your report \ud83d\udcf8</p>"},{"location":"command_line_hpc/#about-files-directories-and-storage-usage","title":"About files, directories and storage usage","text":"<p>A few more commands to try:</p> <ul> <li>List the files with <code>ls</code> and <code>ls -l</code></li> <li>Explore the directories with <code>cd</code></li> <li>Do you know the command <code>tree -d</code>? Check the man page to understand what it does <code>man tree</code>.</li> <li>To know your storage usage and quota: <code>df-ulhpc</code>. It is an enhanced version of the <code>df</code> tool specific to the Uni.lu HPC.</li> <li>To know more about the size of specific files and directories, you can use <code>du</code> or <code>ncdu</code>. For example, try <code>du -sh ~</code> and <code>ncdu ~</code> to know about your home directory.</li> </ul> Disk usage and number of files <p>In Linux, understanding disk usage and number of inodes is crucial for managing your filesystem effectively.</p> <p>Disk Usage:</p> <ul> <li>Represents the actual physical space occupied by your files and folders.</li> <li>Measured in units like kilobytes (KB), megabytes (MB), gigabytes (GB), etc.</li> <li>You can check disk usage with commands like <code>df</code>, <code>du</code> or <code>df-ulhpc</code>.</li> </ul> <p>Number of Inodes:</p> <ul> <li>Represents the number of entries available to store information about files and directories, even if they're empty.</li> <li>Think of it like unique IDs for each file/folder, similar to house numbers.</li> <li>Fixed when the filesystem is created, cannot be changed dynamically.</li> <li>Check inode usage with <code>df -i</code>, <code>du --inodes</code> or <code>df-ulhpc -i</code>.</li> </ul>"},{"location":"command_line_hpc/#type-of-commands-and-files","title":"Type of commands and files","text":"<p>The shell is the program that reads the command line typed by the user, executes it and displays the output. Bash commands fall into four main categories:</p> <ol> <li>Builtins: These are built-in functionalities of the Bash shell itself, like <code>cd</code> or <code>echo</code>. They are fast and readily available without needing separate files or definitions.</li> <li>Aliases: These are user-defined shortcuts for longer or more complex commands. For example, you could create an alias <code>ll</code> to run <code>ls -l</code> for a detailed directory listing. Aliases are simple and convenient but offer limited functionality.</li> <li>Functions: These are more elaborate user-defined commands, similar to mini-programs within your shell. They can accept arguments, perform calculations, and even call other commands. Functions provide more control and reusability than aliases.</li> <li>Files: Any executable file can be executed as a command. This includes shell scripts, compiled programs, or even plain text files with appropriate permissions. Files offer maximum flexibility but require creation and proper setup.</li> </ol> <p>To understand the type of a command, you can use</p> <ul> <li><code>type</code> to determine the type of a command</li> <li><code>file</code> to determine the type of a file</li> <li><code>which</code> to see the full path of a command</li> </ul> <p>Question 3</p> <p>Find the type, and if applicable the file type, of the following commands: <code>history</code>, <code>echo</code>, <code>cp</code>, <code>srun</code>, <code>si</code>, <code>python3</code>, <code>less</code>, <code>zless</code>, and <code>aionstat</code>.</p>"},{"location":"command_line_hpc/#part-3-setting-up-git-and-downloading-the-materials","title":"\ud83d\udd35 Part 3: Setting up Git and downloading the materials","text":"<p>This practical relies on Git Classroom to distribute the extra materials and for the submission of the report.</p> <p>To create your copy of the repository, follow the invitation link and accept the assignment:</p> <ul> <li>Assignment link: https://classroom.github.com/a/i8EMSXuK</li> </ul> <p>Accessing GitHub from the Uni.lu HPC</p> <p>To be able to clone, pull and push your GitHub repository from the Uni.lu HPC, you need to configure your SSH keys. </p> <ol> <li>Generate a new SSH key on the HPC (if necessary). Normally, you already have one named <code>~/.ssh/id_ecdsa.pub</code></li> <li>Add it to your GitHub account, in the Settings -&gt; SSH and GPG Keys page</li> </ol> <p>If needed, follow the instructions in the GitHub documentation Adding a new SSH key to your GitHub account</p> Organize your files: Create directories and subdirectories in your Home directory! <pre><code>mkdir MHPC-HPC_Software_Environment\ncd MHPC-HPC_Software_Environment\nmkdir Practical_W01\ncd Practical_W01\n</code></pre> <p>Once you have accepted the assignment and configured your GitHub account, you can clone your repository in the Home directory of the cluster.</p> <p><pre><code># Don't forget to replace your GitHub username\ngit clone git@github.com:MHPC-HPCSoftwareEnvironment/w01-command-line-environment-on-hpc-GITHUBUSERNAME.git\n</code></pre> Then you can check the content of the directory:</p> <p></p> Failing to clone? <p>If you fail to clone your GitHub repository, check carefully the error messages.</p> <ul> <li><code>ERROR: Repository not found</code>: Most likely you forgot to replace <code>GITHUBUSERNAME</code> in the command line above. Possibly, you do not accept the assignment after click the GitHub Classroom link.</li> <li><code>Permission Denied</code>: You need to grant access to your GitHub repositories from your Uni.lu HPC account. Check the instruction above in \"Accessing GitHub from the Uni.lu HPC\".</li> </ul>"},{"location":"command_line_hpc/#first-commit-and-push","title":"First commit and push","text":"<p>Let's make sure that your configuration is working.</p> <ol> <li>Go to the directory of your GitHub Classroom;</li> <li>Edit the <code>README.md</code> file, with your favorite text editor</li> <li>Add your name at the top, for example, <code>Student: Firstname Lastname</code></li> <li>Save and exit the text editor</li> <li>Add the modification to Git: <code>git add README.md</code></li> <li>Commit the changes with <code>git commit</code> and set a descriptive message</li> <li>Push your changes to GitHub using <code>git push</code></li> </ol> <p>Check your GitHub repository online to make sure this was successful: https://github.com/MHPC-HPCSoftwareEnvironment/w01-command-line-environment-on-hpc-XXX</p> <p>Editing files on the HPC</p> <p>In a terminal environment, editing a file can appear complicated. Rather than download-edit-upload (which is cumbersome) or using a graphical editor (which can be slow), we usually prefer to use a terminal-based text editor.</p> <p>There are widely known and adopted text editors available on the HPC:</p> <ul> <li>Nano: A user-friendly editor designed for simplicity and ease of use, ideal for beginners or quick edits. To get started: <code>nano &lt;filename&gt;</code>.</li> <li>Vi/Vim: This powerful editor has a steep learning curve but offers immense customization and efficiency, making it a favorite among programmers and experienced users. To get started: <code>vim &lt;filename&gt;</code>.</li> </ul> <p>If you don't know what to pick, use Nano.</p>"},{"location":"command_line_hpc/#part-4-using-a-compute-node-interactively","title":"\ud83d\udd35 Part 4: Using a compute node interactively","text":""},{"location":"command_line_hpc/#information-about-the-computing-platform","title":"Information about the computing platform","text":"<p>The documentation of Uni.lu HPC describes the available partitions and QoS defined on the cluster:</p> <ul> <li>Uni.lu Slurm Partitions</li> </ul> <p></p> <ul> <li>Uni.lu Slurm QOS</li> </ul> <p></p> <p>This information can be retrieved directly from the command line.  In addition, it is also possible to get the status of the queue.</p> <ul> <li>Configuration of the partitions: <code>scontrol show partitions</code> </li> <li>Configuration of the QoS: <code>sacctmgr show qos</code></li> <li>View the jobs in the queue: <code>squeue</code></li> </ul> <p>The last command can be a bit verbose due to all the jobs running and pending on the cluster. It can be filtered using different command line options:</p> <ul> <li>Filter by users: <code>--user</code> or <code>-u</code></li> <li>Filter by state: <code>--states</code> or <code>-t</code></li> <li>Filter by partitions: <code>--partition</code> or <code>-p</code></li> <li>Filter by QoS: <code>--qos</code> or <code>-q</code></li> </ul> <p>For example, <code>squeue -t RUNNING -p interactive</code> will display only the jobs currently running on the interactive partition.</p> <p>Question 4</p> <p>What is the command to see your pending jobs?</p> Tip <p>The command <code>sq</code> command will only your jobs in the queue. This command is specific to the Uni.lu HPC platform.</p>"},{"location":"command_line_hpc/#finding-your-reservation","title":"Finding your reservation","text":"<p>The HPC platform is shared by all its users. All users can submit jobs at any time. There is no guarantee that a compute node will be available immediately for your job to be executed, and you might have to wait in the queue for minutes, hours or even days for larger jobs.</p> <p>To avoid this issue, compute nodes have been reserved for you and are available during the practical sessions of this course.</p> <p>Use the command <code>scontrol show reservation</code> to see the reservations. Find the name (starting with <code>hpc_software_</code>) of the one available today for your usage.</p> <p></p> <p>By adding the <code>--reservation=hpc_software_XX</code> option to your SLURM commands, you will have access to a compute node immediately.</p> <p>Keep in mind</p> <p>The reservations are only valid for a given time, usually the duration of the practical session.</p> <ul> <li>During the practicals, use the reservations as those nodes are dedicated to you.</li> <li>Outside the practical hours, you cannot use the reservations. Simply remove the <code>--reservation</code> option from the <code>salloc</code>/<code>sbatch</code> command line to access the compute nodes like every other user.</li> </ul>"},{"location":"command_line_hpc/#connection-to-a-compute-node","title":"Connection to a compute node","text":"<p>To use a compute node, we use the <code>salloc</code> command:</p> <pre><code>salloc -p interactive --qos debug --reservation=hpc_software_d20 --time=1:00:00\n</code></pre> <p>Once in the shell on the compute node, you can exit with the <code>exit</code> command or Ctrl+D.</p> <p>Under the hood, <code>salloc</code> performs many operations:</p> <ul> <li>allocate CPUs on the available compute nodes</li> <li>connect to the first node of the allocation</li> <li>execute a shell by default</li> <li>release the allocation once the shell exits</li> </ul> <p>It is a blocking process: the <code>salloc</code> finishes only once the shell running on the compute node is terminated. Furthermore, the partition <code>interactive</code> and the QoS <code>debug</code> request resources that are dedicated and adapted for interactive usage: a small number of resources for a short time but with a high priority.</p> <p>Additional command line options can specified with <code>salloc</code>:</p> <ul> <li>Number of nodes: <code>-N</code> or <code>--nodes</code></li> <li>Number of tasks or processes: <code>-n</code> or <code>--ntasks</code></li> <li>Number of cores per process: <code>-c</code> or <code>--cpus-per-task</code></li> <li>Time limit: <code>-t</code> or <code>--time</code></li> </ul> Tip <p>The command <code>si</code> is a quick shortcut to get a compute interactively. This command is specific to the Uni.lu HPC platform.</p> <p>More details are given in the documentation.</p> <p>Question 5</p> <p>Once connected to a compute node:</p> <ul> <li>How can you tell you're now connected to a compute node and not the access node?</li> <li>What is the name of the compute node you're connected to?</li> <li>Use the <code>sq</code> to see your job in the queue</li> <li>What is your JOBID?</li> <li>What is the total duration of your job?</li> </ul> <p>Add a screenshot of the relevant part of your terminal  \ud83d\udcf8</p>"},{"location":"command_line_hpc/#finding-and-using-software","title":"Finding and using software","text":"<p>Once connected to a compute node, it is possible to look for the available software and use them with the <code>module</code> command. Try the following commands:</p> <pre><code># Overview of the available software\nmodule overview\n# List and version of available software\nmodule avail\n</code></pre> Error when executing <code>module</code>? <p>The <code>module</code> command is not available on the access node to avoid running computing tasks on by mistake.</p> <ul> <li>Check that you are correctly connected to a compute node.</li> </ul> <p>With <code>module</code>, we can find and use a more recent GCC compiler:</p> <pre><code># Check the current GCC\nwhich gcc\ngcc --version\n\n# Look for GCC compiler\nmodule avail compiler/GCC\n# Load the GCC module\nmodule load compiler/GCC/10.2.0\n# Check the loaded modules\nmodule list\n# Check the loaded GCC\nwhich gcc\ngcc --version\n\n# Unload GCC\nmodule unload compiler/GCC/10.2.0\nmodule list\n# Unload all modules\nmodule purge\nmodule list\n</code></pre>"},{"location":"command_line_hpc/#running-a-parallel-application","title":"Running a parallel application","text":"<p>With this exercise, we will run a small parallel simulation of Computational Fluid Dynamics (CFD) using the OpenFOAM software. This toy example calculates the airflow around a motorbike.</p> <p></p> <p>Let's make sure to allocate at least 6 CPU cores for this task.</p> <pre><code>salloc -p interactive --qos debug --reservation=hpc_software_d20 --time=1:00:00 -n 1 -c 6\n</code></pre> <p>Find and load the OpenFOAM application, and set up the environment:</p> <pre><code>module avail OpenFOAM\nmodule load cae/OpenFOAM/8-foss-2020b\nsource $FOAM_BASH\n</code></pre> <p>Copy the simulation input files from the cloned Git repository.  Make the copy outside the repository as it is not intended to be added to the repository</p> <pre><code>cp -r w01-command-line-environment-on-hpc-XXX/motorBike-INPUT/ ./motorBike-INTERACTIVE\ncd ./motorBike-INTERACTIVE\n</code></pre> <p>Test the parallel execution of the simulation in an interactive mode:</p> <pre><code># Decompose the problem for parallel execution (specific to OpenFOAM)\ndecomposePar\n\n# Run the OpenFOAM solver in parallel: 6 processes with 1 core each\nsrun -n 6 -c 1 simpleFoam -parallel\n</code></pre> <p>The execution should last around 2 minutes. It will display a lot of output showing the progress of the simulation. If successful, it will end with the line <code>Finalising parallel run</code>. </p> <p>Congratulation! You have executed a CFD parallel simulation on the HPC cluster of the University \ud83c\udf89</p> <p>You can now disconnect from the compute nodes using the <code>exit</code> command.</p>"},{"location":"command_line_hpc/#part-5-submitting-an-hpc-job","title":"\ud83d\udd35 Part 5: Submitting an HPC job","text":"<p>In this part, you will learn how to submit a passive job (also called batch). These jobs are executed in the background when resources are available. There is no need to stay connected to the platform.</p>"},{"location":"command_line_hpc/#specifying-resources","title":"Specifying resources","text":"<p>One of the key aspects of SLURM is the specification of resources.  Many options allow to you describe with precision and flexibility the requirements for your parallel application.</p> <p>This table summarizes the main options for <code>salloc</code>, <code>sbatch</code> and <code>srun</code>:</p> SLURM Option SLURM Terminology Usual Terminology <code>--nodes</code> / <code>-N</code> Number of nodes Number of compute nodes <code>--ntasks</code> / <code>-n</code> Number of tasks Number of processes <code>--cpus-per-task</code> / <code>-c</code> Number of CPUs per task Number of cores/threads per process <code>--ntasks-per-socket</code> Number of tasks per socket Number of processes per physical CPU <code>--gpus</code> / <code>-G</code> Number of GPUs Number of GPUs <p>There are many other options available to specify the resource constraints in SLURM. Check the SLURM documentation or man pages for more details.</p> <p>Warning</p> <p>SLURM uses sometimes a different terminology than the usual one. It is important to remember the main differences:</p> <ul> <li>A task in SLURM means a process.</li> <li>A CPU in SLURM means a physical CPU core.</li> <li>A socket in SLURM refers to a physical Socket, i.e., to the whole CPU or processor.</li> </ul>"},{"location":"command_line_hpc/#writing-a-submission-script-for-slurm","title":"Writing a submission script for SLURM","text":"<p>A batch job is submitted using the <code>sbatch</code> command and a script. SLURM interpret the resource constraints to schedule the job and when the resources are available, the script is executed.</p> <p>The resource constraint can be specified on the command line after the <code>sbatch</code> command (similarly to what we have done for <code>salloc</code>) or inside the script using the <code>#SBATCH</code> keyword at the top of the script. It is also possible to use a combination of both, in which case the command line options take priority.</p> <p>Now, we will write a submission script for the motorbike simulation that we tested earlier. An example script is provided simulation input directory.</p> <p>Let's first make a new copy of the input directory.</p> <pre><code>cp -r w01-command-line-environment-on-hpc-XXX/motorBike-INPUT/ ./motorBike-RUN-06\ncd ./motorBike-RUN-06\n</code></pre> <p>Inside this directory, there is a script called <code>run_OpenFOAM_parallel.sh</code>. We will modify it and adapt it to our needs.</p> <pre><code>#!/bin/bash -l\n#SBATCH --job-name motorbike\n#SBATCH --time 0-00:10:00\n#SBATCH --nodes 1\n#SBATCH --ntasks 6\n#SBATCH --cpus-per-task 1\n#SBATCH --partition batch\n#SBATCH --qos normal\n#SBATCH --output SLURM_%x_%j.log\n#SBATCH --error  SLURM_%x_%j.log\n\necho \"== Starting job ${SLURM_JOBID} at $(date)\"\n\n# Load OpenFOAM\nmodule load cae/OpenFOAM/8-foss-2020b\nsource $FOAM_BASH\n\n# Go to the working directory (UPDATE ME!)\ncd /home/users/XXXXX/motorBike-RUN-06\n\n# Set the number of processes in OpenFOAM settings\nfoamDictionary -entry numberOfSubdomains -set \"${SLURM_NTASKS}\" system/decomposeParDict\n\n# Decompose the problem for parallel execution\ndecomposePar\n\n# Run the OpenFOAM solver in parallel\ntime srun -n \"${SLURM_NTASKS}\" -c 1 simpleFoam -parallel\n\necho \"== Finished job ${SLURM_JOBID} at $(date)\"\n</code></pre> <p>Use your favorite text editor to edit the script:</p> <ul> <li>Change the working directory at line 19</li> </ul> <p>Read the script line by line to understand what it does. It is very similar to what we have done manually to run the parallel simulation.</p>"},{"location":"command_line_hpc/#job-submission","title":"Job submission","text":"<p>The script submission is done using the simple <code>sbatch</code> command: <pre><code>sbatch run_OpenFOAM_parallel.sh\n</code></pre></p> <p>However, to use the reserved nodes during the practical session, you should rather use: <pre><code>sbatch --reservation=hpc_software_d20 run_OpenFOAM_parallel.sh\n</code></pre></p>"},{"location":"command_line_hpc/#job-monitoring","title":"Job monitoring","text":"<p>It is possible to monitor the execution of your job.</p> <p>Before the job starts:</p> <ul> <li>Using <code>sq</code> will show the status of your job. The column ST indicates the state: <code>P</code> for pending, <code>R</code> for running.</li> </ul> <p>Once the job is running:</p> <ul> <li>The command <code>sq</code> will indicate on which compute node it is running. You can use SSH to connect directly to this node and monitor the processes. Try <code>top</code> or <code>htop</code> to monitor the CPU usage. </li> <li>The terminal output of the execution is saved in the log file specified by the <code>--output</code> and <code>--error</code> options. Open this file with <code>less</code> or <code>tail -f</code> to see the progress of the execution.</li> </ul> <p>After the execution:</p> <ul> <li>The job will disappear from the <code>sq</code> output, which means the execution is completed (with or without errors). Check the log file to find out if the execution was successful.</li> <li>At the end of the log file, there will be time indicated. It is the time spent in executing the parallel simulation, measured and displayed using the <code>time</code> command at line 28 of the script.</li> </ul> <p></p>"},{"location":"command_line_hpc/#using-more-computing-resources","title":"Using more computing resources","text":"<p>Now, we want to run the simulation with more computing cores:</p> <ol> <li>Create two new directories from the initial input:<ul> <li><code>motorBike-RUN-12</code> for an execution with 12 cores</li> <li><code>motorBike-RUN-24</code> for an execution with 24 cores</li> </ul> </li> <li>Modify the batch script in each directory:<ul> <li>Don't forget to modify the number of processes and the working directory</li> </ul> </li> <li>Submit the two jobs with <code>sbatch</code><ul> <li>Monitor the executions to ensure it is successful</li> </ul> </li> </ol> <p>Question 6</p> <ul> <li>Collect the execution time of the parallel simulations for 6, 12 and 24 cores. </li> <li>In your report, create a table with 3 columns: SLURM JOBID, Execution Time and Name of the file log.</li> <li>Copy the three log files in the Git Classroom repository you have cloned. Put them in the <code>logs</code> sub-directory. Don't forget to commit and push.</li> </ul>"},{"location":"command_line_hpc/#part-6-final-questions-and-report-submission","title":"\ud83d\udd35 Part 6: Final questions and report submission","text":"<p>For the last question, I would like to know about your background.</p> <p>Question 7</p> <p>Please answer with one line/sentence to each question:</p> <ul> <li>What Operating System(s) do you use on your personal laptop and/or workstation?</li> <li>Are you familiar with Linux and the Command Line? For how long?</li> <li>What programming languages do you know? What level? (beginner, medium, expert)</li> <li> <p>Do you know MPI, OpenMP, CUDA? Or any other parallel programming model?</p> </li> <li> <p>Why did you choose the Master in HPC?</p> </li> </ul>"},{"location":"command_line_hpc/#final-submission","title":"Final submission \u26a0\ufe0f","text":"<p>Don't forget to add and push all the required files to your GitHub Classroom repository:</p> <ul> <li>the log files of question 6;</li> <li>the report, including the answers to all the questions and the screenshots.</li> </ul> <p>Finalize your report and save it as a PDF (no other format allowed!) in the <code>report</code> sub-directory of your GitHub Classroom repository. </p> Uploading your report <p>You don't need to copy it to the HPC to upload your report. Instead, you can use the Add file button directly on the GitHub webpage.</p> <p>Files Submission</p> <p>Your files and report are considered submitted once they are pushed to your repository. </p> <p>Check your GitHub repository online to make sure: https://github.com/MHPC-HPCSoftwareEnvironment/w01-command-line-environment-on-hpc-XXX</p> <p>Passed the deadline at 25/02/2024 23h59 you won't be able to push anymore.</p>"},{"location":"command_line_hpc/#additional-resources","title":"\ud83d\udd35 Additional Resources","text":""},{"location":"command_line_hpc/#getting-help","title":"Getting help","text":"<p>Uni.lu HPC Tutorials and Documentation:</p> <ul> <li>Getting Started on Uni.lu HPC </li> <li>Job scheduling with SLURM</li> </ul> <p>Online Documentation:</p> <ul> <li>Bash Reference Manual</li> <li>Manpages</li> <li>Git Reference Manual</li> <li>SLURM Documentation</li> <li>User\u2019s Tour of the Module Command</li> </ul> <p>Books:</p> <ul> <li>Kochan and Wood, Shell Programming in Unix, Linux and OS X</li> </ul>"},{"location":"command_line_hpc/#dont-fall-behind-self-learning-materials","title":"Don't Fall Behind: Self-learning Materials","text":"<p>Linux and Command Line:</p> <ul> <li>Introduction to Linux in HPC on HPC Wiki</li> <li>The Unix Shell on Software Carpentry</li> </ul> <p>Terminal-based Text Editors:</p> <ul> <li>How to Use Vim \u2013 Tutorial for Beginners on freeCodeCamp</li> <li>Beginner's Guide to Nano Text Editor</li> </ul> <p>Git and GitHub:</p> <ul> <li>Version Control with Git on Software Carpentry</li> <li>GitHub and Git Tutorial for Beginners on DataCamp</li> </ul> <p>HPC Tools and Environment:</p> <ul> <li>Job scheduling with SLURM on Uni.lu HPC</li> <li>Using software modules on Uni.lu HPC</li> </ul>"},{"location":"compilation/","title":"Compilation on HPC","text":"<ul> <li>Author: Xavier Besseron (University of Luxembourg)</li> <li>License: \u00a92024 CC BY-NC-SA 4.0</li> <li>Date of practical: Tuesday 27th of February 2024</li> </ul>"},{"location":"compilation/#introduction","title":"\ud83d\udd35 Introduction","text":"<p>This practical session focuses on the compilation of software in an HPC environment. Rather than relying on pre-built packages, HPC clusters often require to re-compile the software to take advantage of the hardware and software characteristics of the platform and obtain the best performance.</p>"},{"location":"compilation/#objectives","title":"Objectives","text":"<p>This work proposes concrete examples of software compilation on HPC.</p> <ul> <li>Find the available compilers, toolchains and dependencies</li> <li>Compile software and libraries using classical build systems (e.g., Configure, CMake)</li> <li>Try different compilers and compilation options</li> </ul> <p>The exercises will also refer to external documentation and instructions that need to be consulted. The aim is for you to be independent and work in 'real conditions' by finding the information you need to complete the task.</p>"},{"location":"compilation/#instructions","title":"Instructions","text":"<p>This practical is an individual work to be carried out on the Aion cluster of the HPC platform of the University of Luxembourg.</p> <p>It is composed of different elements:</p> <ul> <li>this page that contains instructions and questions;</li> <li>the GitHub Classroom repository that contains the materials for the exercises;</li> <li>the report that you have to submit via GitHub Classroom with the rest of the requested documents</li> </ul>"},{"location":"compilation/#report","title":"Report","text":"<p>Together with this tutorial, you need to prepare a short report of your work:</p> <ul> <li>There is no template, you can start from a blank document. Keep it simple, clear and well-organized.</li> <li>Make sure you answer all the questions (cf below), with text, code or screenshots.</li> <li>Prepare your report at the same time you're doing the exercises.</li> <li>Submit your report as a PDF file to your GitHub Classroom repository and push it before the deadline.</li> </ul> <p>Question 1: What? Who? When?</p> <p>Indicate at the beginning of your report:</p> <ul> <li>the title of the practical</li> <li>your full name</li> <li>your Uni.lu HPC username</li> <li>your GitHub username</li> <li>the date</li> </ul>"},{"location":"compilation/#part-0-setup","title":"\ud83d\udd35 Part 0: Setup","text":""},{"location":"compilation/#connection-to-a-compute-node","title":"Connection to a Compute Node","text":"<p>The exercises are to be carried out in interactive mode on a compute node of the Aion cluster of the University of Luxembourg.</p> <p>As a reminder, here are the steps to access a compute nodes:</p> <ul> <li>Connect to the access node of the Aion cluster (cf Connection to HPC Access)</li> <li>Find your reservation, if application (cf Finding your reservation)</li> <li>Connect to a compute node (cf Connection to a compute node)</li> </ul> Recommended Settings <p>For this practical session, the following options are suggested:</p> <ul> <li>Partition: <code>interactive</code></li> <li>QoS: <code>debug</code></li> <li>Reservation: <code>hpc_software_d27</code> during the Tuesday afternoon session</li> <li>Time: 2 hours (or ending before the end of the reservation)</li> <li>Node: One compute node</li> <li>Task: One task</li> <li>Cores: 32 cores per task</li> </ul> <p>Your access command line should look like this:</p> <pre><code>salloc -p interactive --qos debug --reservation=hpc_software_d27 --time=2:00:00 -N 1 -n 1 -c 32\n</code></pre>"},{"location":"compilation/#github-classroom-repository","title":"GitHub Classroom Repository","text":"<p>Following the same approach as the first week practical, this practical relies on Git Classroom to distribute the extra materials and for the submission of the report.</p> <p>To create your copy of the repository, follow the invitation link and accept the assignment:</p> <p>Assignment link: https://classroom.github.com/a/0gg2p9aK</p> <p>If you need detailed explanations, please refer to the first week's instructions for Setting up Git and downloading the materials.</p>"},{"location":"compilation/#part-1-compilers-and-toolchains","title":"\ud83d\udd35 Part 1: Compilers and Toolchains","text":"<p>From now on, it is assumed that you're connected to a compute node of the Uni.lu HPC platform.  To make sure of that, check the shell prompt: It should end with the name of the node you're connected to, for example, <code>aion-0090</code>.</p>"},{"location":"compilation/#finding-available-compilers","title":"Finding available compilers","text":"<p>As seen previously, <code>module</code> is the command allowing one explore and find software on an HPC platform. But before that, let's check if there is a 'system' compiler already installed.</p> <p>For that, we can check the usual names for compilers: <code>cc</code> and <code>gcc</code>.</p> <p>Question 2: Finding system compilers</p> <p>Check the commands <code>cc</code> and <code>gcc</code>:</p> <ul> <li>What is the type of these commands? What is their full path? (you can use the commands <code>which</code>, <code>type</code>, <code>file</code> and/or <code>ls -l</code> to do that)</li> <li>What is the name of these compilers? What version? (you can use the command line option <code>--version</code> to do that)</li> <li>Are they the same compiler? Why?</li> </ul> Warning <p>If you already have loaded modules, clear them off with <code>module purge</code> to make sure you only find system compilers.</p> <p>The system compiler is the one that comes packaged with the Linux distribution installed on the computing node. It is usually not used to install HPC software because it is old.  Other and more recent compilers are available via the modules. Let's check them out.</p> <p>To find compilers among the modules, we can proceed in different ways:</p> <ol> <li>Search by name when you already know what you're looking for. For example, in the lecture we have seen compilers <code>gcc</code>, <code>icc</code> and <code>clang</code>;</li> <li>Search by module class, which sometimes appears as a prefix of the module name;</li> <li>Search in the description of the module.</li> </ol> <p>The search can sometimes be complicated because there is no naming convention between the HPC sites.  For example, on Uni.lu HPC, the modules are named <code>&lt;class&gt;/&lt;name&gt;/&lt;version&gt;</code>. However, on MeluXina, the modules are named <code>&lt;name&gt;/&lt;version&gt;</code>.  Furthermore, the <code>&lt;version&gt;</code> part may also contain the name of the toolchain used to compile the software. Because of that, you might need to adapt your search pattern on other sites.</p> <p>Here are some concrete examples of how to find compilers on Aion:</p> <pre><code># Overview of all modules\nmodule overview\n\n# Find all modules whose name contains 'compiler'\nmodule avail compiler\n\n# Find all modules with class 'compiler'\nmodule -r avail '^compiler/'\n\n# Find modules with 'compiler' in their name or description\nmodule keyword compiler\n\n# Find all modules that are exactly named 'GCC'\nmodule avail '/GCC/'\n\n# Find all modules that are exactly named 'iccifort' (for Intel compiler)\nmodule avail '/iccifort/'\n</code></pre> <p>We will now focus only on the following C/C++ compilers:</p> Compiler Module Name C Compiler Command C++ Compiler Command Fortran Compiler Command GNU Compiler Collection (GCC) GCC <code>gcc</code> <code>g++</code> <code>gfortran</code> LVVM C, C++, Objective-C compiler Clang <code>clang</code> <code>clang++</code> <code>flang</code> Intel C, C++ &amp; Fortran compilers iccifort <code>icc</code> <code>icpc</code> <code>ifort</code> AMD Optimized C/C++ &amp; Fortran compilers (AOCC) AOCC <code>clang</code> <code>clang++</code> <code>flang</code> <p>Question 3: Finding compiler modules</p> <p>For each of the compilers in the table above, find them on the Aion cluster and write down:</p> <ul> <li>the full module name, with class-name-version;</li> <li>the full path of the C compiler;</li> <li>the version returned by the C compiler executable (use the <code>--version</code> command line option).</li> </ul>"},{"location":"compilation/#finding-available-toolchains","title":"Finding available toolchains","text":"<p>A compiler toolchain (or toolchain for short) is a set of software, tools and libraries, that are commonly used together to build applications. It typically consists of one or more compilers (for C, C++ and Fortran), an MPI library and an optimized linear algebra library implementing the BLAS/LAPACK interface.</p> <p>The software stack on the Uni.lu HPC platform is installed using EasyBuild (to be introduced in the coming weeks)  and in consequence, the toolchains available on Aion are a subset of the ones defined by EasyBuild.</p> <p>Currently, on Aion, we find the compilers and toolchains as defined in this diagram. At the top level, we find two toolchains:</p> <ul> <li><code>foss</code> (current version <code>2020b</code> on Uni.lu HPC) based on free and open-source software;</li> <li><code>intel</code> (current version <code>2020b</code> on Uni.lu HPC) based on Intel software.</li> </ul> <pre><code>graph LR\n  A[GCCCore] --&gt; |binutils| B[GCC];\n  A --&gt; |binutils| C[iccifort]\n  B --&gt; |OpenMPI| E[gompi];\n  C --&gt; |\"Intel MPI (impi)\"| F[iimpi];\n  E --&gt; |OpenBLAS + FFTW + ScaLAPACK| G[foss];\n  F --&gt; |\"Intel MLK (imkl)\"| H[intel];</code></pre> Newer toolchain definitions in EasyBuild <p>EasyBuild has redefined its toolchains in its recent versions (not yet available on Uni.lu HPC). You can find more details in the documentation.</p> <p>Same as before for the compilers, you can find and use the toolchains using the <code>module</code> command:</p> <pre><code># List the modules of the 'toolchain' class\nmodule avail 'toolchain/'\n</code></pre> <p>Question 4: Finding toolchains</p> <p>List the main components of the main toolchains:</p> <ul> <li>For <code>toolchain/foss/2020b</code>: the module name and version of the compiler, MPI, BLAS, LAPACK and FFTW;</li> <li>For <code>toolchain/intel/2020b</code>: the module name and version of the compiler, MPI, and MKL.</li> </ul> <p>You can either use <code>module show</code> to inspect a module. Alternatively, you can load the module and use <code>module list</code> to display the loaded modules.</p>"},{"location":"compilation/#part-2-hello-world","title":"\ud83d\udd35 Part 2: Hello World!","text":"<p>In this part, let's do a simple compilation of \"Hello World!\" programs using the FOSS toolchain <code>foss/2020b</code>. Two sample programs are available in the <code>hello_world</code> directory of your GitHub classroom repository:</p> <ul> <li><code>hello.cpp</code>: a pure C++ program;</li> <li><code>hello_mpi.cpp</code>: a C++ program using MPI.</li> </ul> <p>To compile and use these programs, follow these steps:</p> <ol> <li>Load the <code>toolchain/foss/2020b</code> toolchain</li> <li>Compile <code>hello.cpp</code> into the executable <code>hello</code> using the appropriate C++ compiler</li> <li>Execute the <code>hello</code> program</li> <li>Compile <code>hello_mpi.cpp</code> into the executable <code>hello_mpi</code> using the appropriate C++ MPI compiler (cf the lecture)</li> <li>Execute the <code>hello_mpi</code> program</li> </ol> Errors? <ul> <li>Error <code>undefined reference to 'std::cout'</code>: make sure you're using a C++ compiler</li> <li>Error <code>undefined reference to 'MPI_Init'</code>: make sure you're using a MPI compiler</li> <li>Error <code>bash: hello: command not found</code>: run the executable with <code>./hello</code> explicitely</li> </ul> <p>Question 5: Compiling HelloWorld! with <code>foss</code> toolchain</p> <p>Compile the \"Hello World!\" program with the FOSS toolchain:  Follow the five steps above and add a screenshot to your report \ud83d\udcf8</p> <p>The screenshot must show the input command lines and the output of the five steps, without errors.</p> <p>Now, let's try again the compilation of the \"Hello World!\" programs with the Intel toolchain <code>intel/2020b</code>. Adapt the five steps above to use the Intel toolchain and compilers. Make sure to clear all the loaded modules with <code>module purge</code> first.</p> <p>Question 6: Compiling HelloWorld! with <code>intel</code> toolchain</p> <p>Compile the \"Hello World!\" program with the Intel toolchain:  Adapt the five steps above and add a screenshot to your report \ud83d\udcf8</p> <p>The screenshot must show the input command lines and the output of the five steps, without errors.</p>"},{"location":"compilation/#part-3-compilation-of-the-gnu-mpfr-library","title":"\ud83d\udd35 Part 3: Compilation of the GNU MPFR Library","text":"<p>The GNU MPFR library stands for Multiple Precision Floating-Point Reliable library and is a portable C library for arbitrary-precision binary floating-point computation with correct rounding, based on the GNU Multi-Precision (GMP) library. It is used in many scientific software, including Maple, GNU Octave or the Julia language.</p> <ul> <li>Homepage</li> <li>Download Source</li> <li>Installation Instructions</li> </ul> <p>The build system of MPFR is based on the Autotools with a classical configure-make-make install workflow with a <code>configure</code> script. For this part, your task is to compile and install the latest version of the library (currently 4.2.1) with the following settings:</p> <ul> <li>use the native processor architecture of the Aion cluster and optimization level 3;</li> <li>generate thread-safe code.</li> </ul> <p>Additional Information</p> <p>Minimal guidance is provided for you to complete this exercise, but keep in mind this information:</p> <ul> <li>MPFR is a C library, hence you will need to load a C compiler (GCC recommended).</li> <li>It also depends on the GMP library: use the one available in the modules.</li> <li>The source code can be downloaded from the Download Page. Use <code>wget '&lt;link&gt;'</code> to download from the command line directly on the compute node. </li> <li>Use the <code>tar</code> command to extract the source code, for example, <code>tar -xzf mpfr-4.2.1.tar.gz</code> (more help online, for example here).</li> <li>MPFR is based on Autotools, so you will need to run the <code>configure</code> script to configure the build. Check also the Installation Instructions from the website.</li> <li>You can set the optimization level and target architecture using the appropriate environment variable (as explained in the lecture).</li> <li>Options for the thread-safe build and the GMP dependency can be found with the <code>--help</code> of <code>configure</code>.</li> <li>Install the library somewhere appropriate in your home directory, for example, a sub-directory dedicated to this practical.</li> </ul> <p>The installation procedure can follow this pattern:</p> <pre><code># Use a dedicated work directory\nmkdir mpfr ; cd mpfr\n\n# Download the source code\nwget '&lt;link&gt;'\n\n# Extract the source code\ntar -xzf mpfr-4.2.1.tar.gz\n\n# Create a build directory\nmkdir build ; cd build\n\n# Load the required modules\nmodule load '&lt;module names ...&gt;'\n\n# Set the environment variables for the compilation options\nexport '&lt;VARIABLE&gt;'='&lt;options&gt;'\n\n# Configure the build\n# - set the install path\n# - set the configure option\n'&lt;configure command line with options&gt;'\n\n# Build the library (and save output)\nmake -j 8 |&amp; tee build.log\n\n# Install the library (and save output)\nmake install |&amp; tee install.log\n</code></pre> What is <code>command |&amp; tee file.log</code>? <p>This syntax executes <code>command</code>, displays its output in the terminal and, at the same time, saves it to <code>file.log</code>.</p> <p>In more details, we have:</p> <ul> <li><code>command</code> to execute the command;</li> <li><code>|&amp;</code> is a shell pipe to redirect both the standart output and the error output (the usual pipe <code>|</code> only uses the standard output);</li> <li><code>tee</code> is a small program that takes its input (coming from the pipe in this context) to displays it to the terminal and save it to a file;</li> <li><code>file.log</code> is the parameter of the <code>tee</code> command indicating to which file the output should be saved.</li> </ul> <p>Question 7: Installation procedure for the MPFR library</p> <p>Fill the missing holes in the installation procedure for the MPFR library:</p> <ul> <li>write down the complete working installation procedure in your report</li> <li>add the files <code>config.log</code>, <code>build.log</code> and <code>install.log</code> (and only these files) to your GitHub classroom repository in the <code>mpfr</code> directory</li> </ul>"},{"location":"compilation/#part-4-compilation-of-the-precice-library","title":"\ud83d\udd35 Part 4: Compilation of the preCICE library","text":"<p>The preCICE library is an open-source coupling library for partitioned multi-physics simulations. It is designed to facilitate the coupling of independent simulation software components.  It's particularly useful for multi-physics simulations, allowing programs specializing in different aspects (like fluid dynamics or structural analysis) to communicate and exchange data for a more comprehensive simulation.</p> <ul> <li>Hompage</li> <li>Download Source</li> <li>Dependencies</li> <li>Installation Instructions</li> </ul> <p></p> <p>The build system of preCICE is based on CMake with a classical cmake-build-install workflow. For this part, your task is to compile and install the latest version of the library (currently 3.0.0) with the following settings:</p> <ul> <li>use the native processor architecture of the Aion cluster and the Release build type;</li> <li>make sure the following options are enabled:<ul> <li>MPI communication</li> <li>Python actions</li> <li>PETSc mapping</li> <li>C bindings</li> <li>Fortran bindings</li> </ul> </li> </ul> <p>Additional Information</p> <p>Minimal guidance is provided for you to complete this exercise, but keep in mind this information:</p> <ul> <li>preCICE is a C++ library with C and Fortran bindings, hence you will need to load a C, C++ and Fortran compilers (GCC recommended).</li> <li>preCICE has many dependencies. Make sure to load the appropriate modules.</li> <li>The source code can be downloaded from the GitHub release page of the project. Use <code>wget '&lt;link&gt;'</code> to download from the command line directly on the compute node.</li> <li>Use the <code>tar</code> command to extract the source code, for example, <code>tar -xzf v3.0.0.tar.gz</code> (more help online, for example here).</li> <li>preCICE is based on CMake, so you will need to run the <code>cmake</code> command to configure the build. Check also the Installation Instructions from the website.</li> <li>You can set the target architecture using the appropriate environment variable (as explained in the lecture).</li> <li>You can set the build type using the standard CMake option (as explained in the lecture).</li> <li>Check the Installation Instructions to set the required build options.</li> <li>Install the library somewhere appropriate in your home directory, for example, a sub-directory dedicated to this practical.</li> </ul> <p>The installation procedure can follow this pattern:</p> <pre><code># Use a dedicated work directory\nmkdir precice ; cd precice\n\n# Download the source code\nwget '&lt;link&gt;'\n\n# Extract the source code\ntar -xzf v3.0.0.tar.gz\n# Create a build directory\nmkdir build ; cd build\n\n# Load the required modules\nmodule load '&lt;module names ...&gt;'\n\n# Set the environment variables for the compilation options\nexport '&lt;VARIABLE&gt;'='&lt;options&gt;'\n\n# Configure the build\n# - set the install path\n# - set the build type\n# - set the cmake option to enable MPI communications, Python actions, PETSc mapping, C and Fortran bindings\n'&lt;cmake command line with options&gt;' |&amp; tee cmake.log\n\n# Build the library (and save output)\nmake -j 16 |&amp; tee build.log\n\n# Install the library (and save output)\nmake install |&amp; tee install.log\n</code></pre> <p>Question 8: Installation procedure for the preCICE library</p> <p>Fill the missing holes in the installation procedure for the preCICE library:</p> <ul> <li>write down the complete working installation procedure in your report</li> <li>add the files <code>cmake.log</code>, <code>build.log</code> and <code>install.log</code> (and only these files) to your GitHub classroom repository in the <code>precice</code> directory</li> </ul>"},{"location":"compilation/#part-5-compilation-of-hpl","title":"\ud83d\udd35 Part 5: Compilation of HPL","text":"<p>HPL is a software package that solves a (random) dense linear system in double precision (64 bits) arithmetic on distributed-memory computers.  It can thus be regarded as a portable as well as freely available implementation of the High-Performance Computing Linpack Benchmark.</p> <p>The Linpack Benchmark measures the floating-point computing power and  it is used to rank the world's most powerful supercomputers in the TOP500 list.</p> <ul> <li>Homepage</li> <li>Download and Installation</li> </ul> <p>The build system of HPC relies on an ad-hoc Makefile. The configuration is done by creating and editing a <code>Make.&lt;arch&gt;</code> file that contains the settings specific to the HPC system.</p> <p>For this part, your task is to compile and run the latest HPL benchmark (currently 2.3) with Intel toolchain. To achieve this, follow these instructions:</p> <ol> <li>Download the HPL archive and extract it</li> <li>Go to the <code>hpl-2.3</code> directory (called the top-level directory)</li> <li>Copy the example configuration file <code>setup/Make.Linux_Intel64</code> to the top-level directory with the new name <code>Make.Aion_Intel64</code></li> <li>In the new <code>Make.Aion_Intel64</code> file, edit the following settings:<ul> <li>Set variable <code>ARCH</code> to <code>Aion_Intel64</code></li> <li>Set variable <code>TOPdir</code> to the path to your top-level directory</li> <li>Set variable <code>OMP_DEFS</code> to <code>-qopenmp</code></li> <li>Add <code>-xHost</code> to the variable <code>CCFLAGS</code></li> </ul> </li> <li>Load the Intel toolchain <code>toolchain/intel/2020b</code></li> <li>Compile HPL with <code>make arch=Aion_Intel64</code></li> </ol> <p>If the compilation is successful, you will find an executable called <code>xhpl</code> in the <code>bin/Aion_Intel64</code> directory. To execute a short test of HPL, follow these steps:</p> <ol> <li>Go to the <code>bin/Aion_Intel64</code> directory</li> <li>Run <code>srun -n 4 ./xhpl</code></li> </ol> <p>Question 9: Compilation and execution of HPL</p> <p>Compile and run HPL with the instructions above. </p> <ul> <li>add the configuration file <code>Make.Aion_Intel64</code> in the <code>hpl</code> directory of your GitHub classroom repository</li> <li>save the output of the HPL execution and add it to the <code>hpl</code> directory your GitHub classroom repository</li> </ul>"},{"location":"compilation/#report-submission","title":"\ud83d\udd35 Report submission","text":"<p>Don't forget to add and push all the required files to your GitHub Classroom repository:</p> <ul> <li>the log files of questions 7, 8 and 9;</li> <li>the report, including the answers to all the questions and the screenshots.</li> </ul> <p>Don't add any other files and directories (for example, build directories, executables, Makefile, etc.). Finalize your report and save it as a PDF (no other format allowed!) in the <code>report</code> sub-directory of your GitHub Classroom repository. </p> Uploading your report <p>You don't need to copy it to the HPC to upload your report. Instead, you can use the Add file button directly on the GitHub webpage.</p> <p>Files Submission</p> <p>Your files and report are considered submitted once they are pushed to your repository. </p> <p>Check your GitHub repository online to make sure: https://github.com/MHPC-HPCSoftwareEnvironment/w02-compilation-on-hpc-XXX</p> <p>Passed the deadline at 03/03/2024 23h59 you won't be able to push anymore.</p>"},{"location":"compilation/#additional-resources","title":"\ud83d\udd35 Additional Resources","text":"<p>On HPC Wiki:</p> <ul> <li>About Tar</li> <li>About Modules</li> <li>About Compilers</li> <li>About Make</li> <li>About CMake</li> </ul> <p>Build software with CMake</p> <ul> <li>Building with CMake</li> <li>An Introduction to Modern CMake: Running CMake</li> <li>CMake FAQ: Running CMake</li> <li>How to Build a CMake-Based Project</li> </ul> <p>Build software with Autotools (configure)</p> <ul> <li>Introduction to GNU Autotools</li> </ul> <p>Compilation with a Makefile</p> <ul> <li>What is a Makefile and how does it work?</li> <li>GNU Make Reference Documentation</li> </ul>"},{"location":"debugging_profiling/","title":"Debugging and Profiling","text":"<ul> <li>Author: Xavier Besseron (University of Luxembourg)</li> <li>License: \u00a92024 CC BY-NC-SA 4.0</li> <li>Date of practical: Tuesday 5th of March 2024</li> </ul>"},{"location":"debugging_profiling/#introduction","title":"\ud83d\udd35 Introduction","text":"<p>This practical session focuses on the debugging and profiling of parallel applications on HPC. Debugging and profiling are both crucial because they directly impact code reliability and efficiency. </p> <ul> <li>Debugging helps pinpoint errors and crashes within the complex, massively parallel code typical of HPC applications.</li> <li>Profiling uncovers performance bottlenecks and helps identify optimization opportunities to ensure that expensive HPC resources are used optimally.</li> </ul> <p>These techniques work together to accelerate code development and ensure that the results generated by HPC are accurate and achieve the best possible performance.</p> <p></p>"},{"location":"debugging_profiling/#objectives","title":"Objectives","text":"<p>This work proposes concrete examples of profiling and debugging parallel applications on HPC:</p> <ul> <li>Performance Characterization</li> <li>Performance Profiling</li> <li>Debugging</li> </ul> <p>The exercises will also refer to external documentation and instructions that need to be consulted. The aim is for you to be independent and work in 'real conditions' by finding the information you need to complete the task.</p>"},{"location":"debugging_profiling/#instructions","title":"Instructions","text":"<p>This practical is an individual work to be carried out on the Aion cluster of the HPC platform of the University of Luxembourg.</p> <p>It is composed of different elements:</p> <ul> <li>this page that contains instructions and questions;</li> <li>the GitHub Classroom repository that contains the materials for the exercises;</li> <li>the report that you have to submit via GitHub Classroom with the rest of the requested documents</li> </ul>"},{"location":"debugging_profiling/#report","title":"Report","text":"<p>Together with this tutorial, you need to prepare a short report of your work:</p> <ul> <li>There is no template, you can start from a blank document. Keep it simple, clear and well-organized.</li> <li>Make sure you answer all the questions (cf below), with text, code or screenshots as requested.</li> <li>Prepare your report at the same time you're doing the exercises.</li> <li>Submit your report as a PDF file to your GitHub Classroom repository and push it before the deadline.</li> </ul> <p>Question 1: What? Who? When?</p> <p>Indicate at the beginning of your report:</p> <ul> <li>the title of the practical</li> <li>your full name</li> <li>your Uni.lu HPC username</li> <li>your GitHub username</li> <li>the date</li> </ul>"},{"location":"debugging_profiling/#part-0-setup","title":"\ud83d\udd35 Part 0: Setup","text":""},{"location":"debugging_profiling/#github-classroom-repository","title":"GitHub Classroom Repository","text":"<p>Following the same approach as the previous practicals, this practical relies on Git Classroom to distribute the extra materials and for the submission of the report. To create your copy of the repository, follow the invitation link and accept the assignment:</p> <p>Assignment link: https://classroom.github.com/a/MJWASTD7</p> <p>If you need detailed explanations, please refer to the first week's instructions for Setting up Git and downloading the materials.</p>"},{"location":"debugging_profiling/#interactive-graphical-session","title":"Interactive Graphical Session","text":"<p>Some of the tools used in this practical are graphical tools that require an X11 server.</p> On Windows: Start an X11 server and enable X11-forwarding <p>If you're using Windows, you need to setup an X11 server and start it before connecting to the HPC. The easiest way is to install and use MobaXterm.  To start the X11 server, click to top right icon.</p> <p></p> <p>Then, if make sure to enable X11-forwarding for your connection to the HPC.</p> <p></p> On MacOS X: Start an X11 server and enable X11-forwarding <p>If you're using MacOS X, you need to setup an X11 server and start it before connecting to the HPC. You can follow the instructions on this page to set it up:</p> <ul> <li>How to install X Window System XQuartz on macOS for ssh X11 forwarding</li> </ul> <p>Then, if make sure to enable X11-forwarding for your connection to the HPC.</p> <pre><code>ssh -X aion-cluster\n</code></pre> On Linux: Enable X11-forwarding <p>If you're using Linux or MacOS, use the <code>-X</code> options on your SSH command to enable X11-forwarding for your connection to the HPC. For example</p> <pre><code>ssh -X aion-cluster\n</code></pre>"},{"location":"debugging_profiling/#connection-to-a-compute-node","title":"Connection to a Compute Node","text":"<p>The exercises are to be carried out in interactive mode on a compute node of the Aion cluster of the University of Luxembourg.</p> <p>As a reminder, here are the steps to access a compute nodes:</p> <ul> <li>Connect to the access node of the Aion cluster (cf Connection to HPC Access)</li> <li>Find your reservation, if application (cf Finding your reservation)</li> <li>Connect to a compute node (cf Connection to a compute node)</li> </ul> Recommended Settings <p>For this practical session, the following options are suggested:</p> <ul> <li>Partition: <code>interactive</code></li> <li>QoS: <code>debug</code></li> <li>Reservation: <code>hpc_software_d05</code> during the Tuesday afternoon session</li> <li>Time: 2 hours (or ending before the end of the reservation)</li> <li>Node: One compute node</li> <li>Tasks: 32 tasks</li> <li>Cores: 1 core per task</li> <li>Enable X11 forwarding: <code>--x11</code></li> </ul> <p>Your access command line should look like this:</p> <pre><code>salloc -p interactive --qos debug --reservation=hpc_software_d05 --time=2:00:00 -N 1 -n 32 -c 1 --x11\n</code></pre> Failed to access a computing node? <p>The message <code>salloc: error: No DISPLAY variable set, cannot setup x11 forwarding</code> indicates that you did not enable X11 forwarding when you first connected to the access node.</p>"},{"location":"debugging_profiling/#part-1-performance-characterization","title":"\ud83d\udd35 Part 1: Performance Characterization","text":""},{"location":"debugging_profiling/#introduction_1","title":"Introduction","text":"<p>For this first exercise, we will do the performance characterization of a parallel Computational Fluid Dynamics simulation based on the OpenFOAM software. This toy example calculates the airflow around a motorbike. It is the same example that you have used in the first practical.</p> <p></p> <p>We will use the Performance Reports analysis tool that provides a summarized view of an application's performance characteristics. It can identify whether the application is primarily:</p> <ul> <li>CPU-bound: Limited by how fast the processor can execute instructions;</li> <li>MPI-bound: Limited by communication overhead in distributed computing environments;</li> <li>I/O-bound: Limited by how fast it can read/write data (e.g., hard disk, network);</li> <li>Python interpreter-bound: Limited by the overhead of running in the Python environment.</li> </ul> <p>The generated reports are concise and readable: they are designed to be easily understood, offering insights even when you don't have direct access to the application's source code.</p> <p>Performance Reports is a commercial tool</p> <p>Performance Reports is part of the Linaro Forge tool suite that was previously known as Arm Forge or Allinea Forge. It is a commercial tool that requires a licence to be used, and thus it is not available on all the HPC platforms.</p> <p>Using Performance Reports on Uni.lu HPC</p> <p>The University of Luxembourg has purchased a license for the Performance Reports tool and it is available to use for the user of the HPC platform. The version installed on the Uni.lu HPC is 20.0.3 which was released by Arm. Thus we will call it Arm Performance Reports.</p> <p>On the Aion and Iris cluster, Arm Performance Reports is available via the module <code>tools/ArmReports/20.0.3</code> and the command line executable is named <code>perf-report</code>.</p>"},{"location":"debugging_profiling/#submission-script","title":"Submission script","text":"<p>Performance Reports can be used in a non-interactive way, simply by prefixing your <code>mpirun</code> command with <code>perf-report</code>. Check <code>perf-report --help</code> for more details and options on how to use it. Don't forget to load the module first.</p> <p></p> <p>To characterize the performance of our application, we will create a submission script.  We will start from the script that we developed during the first practical and adapt it to use Performance Reports. In your GitHub Classroom repository, there is a directory named <code>motorBike-input</code>. It contains the input files and the submission script for the motorbike simulation.</p> <ol> <li>Make a copy of the input directory to a new directory named <code>motorBike-PERFREPORT-06</code></li> <li>In the <code>motorBike-PERFREPORT-06</code> directory, edit the submission script to use Performance Reports:<ul> <li>Use 6 computing cores</li> <li>Load the needed module </li> <li>Set the working directory</li> <li>Adapt the <code>mpirun</code> command line</li> </ul> </li> <li>Submit your script to SLURM with <code>sbatch</code></li> <li>Monitor the execution and check the SLURM log file to ensure the execution is successful.</li> </ol> <p>After successful execution, you will get two new files (named <code>simpleFoam_XXX.html</code> and <code>simpleFoam_XXX.txt</code>) in your execution directory. They both contain the results of the performance analysis and the performance characterization of the application.</p> <ul> <li>The <code>.txt</code> file is suitable for a quick look in the terminal using a tool like <code>less</code>.</li> <li>The <code>.html</code> file offers nicer formatting and is more suitable to look at from a web browser after downloading it to your laptop.</li> </ul> <p>Question 2: Performance Reports of the MotorBike simulation on 6 cores</p> <p>In your GitHub Class repository, add the following files to the <code>perfreport</code> directory:</p> <ul> <li>The adapted submission script that runs the performance analysis of the motorbike simulation on 6 cores;</li> <li>The two performance reports (<code>.txt</code> and <code>.html</code>) of the performance analysis.</li> </ul> <p>Looking at the report, can you tell:</p> <ul> <li>What is the main bottleneck for the performance of this application? Percentage spent?</li> <li>What is the second factor limiting the performance? Percentage spent?</li> <li>How much (in percentage) represents the rest? What does it correspond to?</li> </ul> <p>Thinking in terms of roofline analysis,</p> <ul> <li>Is this application limited by the compute operations or the memory accesses?</li> <li>Would this execution benefit from using more vectorized instructions?</li> </ul> <p>Based on this report, </p> <ul> <li>What is your main recommendation to improve the performance of this simulation?</li> </ul> <p>Justify and write down the answers in your report.</p>"},{"location":"debugging_profiling/#execution-at-a-larger-scale","title":"Execution at a larger scale","text":"<p>Now, let's run the same analysis for an execution with more resources. Create a new directory and adapt the submission script for an execution on 24 cores.</p> <p>Question 3: Performance Reports of the MotorBike simulation on 24 cores</p> <p>In your GitHub Class repository, add the following files to the <code>perfreport</code> directory:</p> <ul> <li>The adapted submission script that runs the performance analysis of the motorbike simulation on 24 cores;</li> <li>The two performance reports (<code>.txt</code> and <code>.html</code>) of the performance analysis.</li> </ul> <p>Looking at the new report,</p> <ul> <li>Describe in a few lines the main changes in the results, compared to the execution on 6 cores.</li> <li>What is the change in the execution time? What is the speedup?</li> <li>Is there a significant change in the time spent in I/O?</li> </ul> <p>Justify and write down the answers in your report.</p>"},{"location":"debugging_profiling/#part-2-performance-profiling","title":"\ud83d\udd35 Part 2: Performance Profiling","text":"<p>For this part, we use Arm/Linaro MAP to profile the same OpenFOAM motorbike simulation. At the difference of Performance Reports, MAP will provide a more detailed trace of the simulation over this execution. To profile an application with MAP, there is no need to modify the code. However, it could display better results if the debug flag <code>-g</code> is enabled during the compilation.</p> <p>MAP is a commercial tool</p> <p>MAP is part of the Linaro Forge tool suite and was previously known as Arm Forge or Allinea Forge. Similarly to Performance Reports, it is a commercial tool that requires a licence to be used, and thus it is not available on all the HPC platforms.</p> <p>Using MAP on Uni.lu HPC</p> <p>The University of Luxembourg has purchased a license for Arm Forge (including MAP and DDT) and it is available to use for the user of the HPC platform. The version installed on the Uni.lu HPC is 20.0.3 which was released by Arm.</p> <p>On the Aion and Iris cluster, Arm MAP is available via the module <code>tools/ArmForge/20.0.3</code> and the command line executable is named <code>map</code>.</p> <p>In practice, there are two phases in the process of performance analysis and optimization:</p> <ul> <li>Phase 1: Profiling (Data Collection). It corresponds to the actual act of running your system while MAP is collecting performance data.</li> <li>Phase 2: Exploration and Analysis of Results. Because the profiling data is often vast and complex, good visualization tools are essential. They help to create a good interpretation of the problems, identifying Bottlenecks and understanding the root causes.</li> </ul> <p>MAP relies on a graphical interface that can be used for both phases: When MAP starts, you can select the program to profile, set the parameters and various options and then start the profiling on all your computing nodes. Alternatively, the profiling phase can be done non-interactively, simply by prefixing the <code>mpirun</code> command line with the <code>map --profile</code> command.</p> <p>Question 4: Interactive vs non-interactive profiling</p> <p>In your opinion, what are the advantages of using non-interactive profiling over directly opening the graphical interface and setting it up manually?</p>"},{"location":"debugging_profiling/#submission-script_1","title":"Submission script","text":"<p>Let's use a non-interactive profiling. You can use <code>map --help</code> to see the command line options. Don't forget to load the module first.</p> <p></p> <p>To profile the motorbike simulation, we will create a submission script.  And again, we will start from the template script available in <code>motorBike-input</code>.</p> <ol> <li>Make a copy of the input directory <code>motorBike-input</code> to a new directory named <code>motorBike-MAP-06</code></li> <li>In the <code>motorBike-MAP-06</code> directory, edit the submission script to use MAP:<ul> <li>Use 6 computing cores</li> <li>Load the needed module </li> <li>Set the working directory</li> <li>Adapt the <code>mpirun</code> command line</li> </ul> </li> <li>Submit your script to SLURM with <code>sbatch</code></li> <li>Monitor the execution and check the SLURM log file to ensure the execution is successful.</li> </ol> <p>After successful execution, you will get one new file (named <code>simpleFoam_XXX.map</code>) in your execution directory. It contains the results of the performance profile of the application. </p> <p>Question 5: Profiling of the MotorBike simulation on 6 cores</p> <p>In your GitHub Class repository, add the following files to the <code>profiling</code> directory:</p> <ul> <li>The adapted submission script that runs the profiling of the motorbike simulation on 6 cores;</li> <li>The result trace (file <code>simpleFoam_XXX.map</code>) of the performance profiling.</li> </ul>"},{"location":"debugging_profiling/#visualizing-the-profiling-results","title":"Visualizing the profiling results","text":"<p>To explore the profiling results, you can open the MAP graphical interface simply by executing the <code>map</code> command in your terminal. Make sure you have loaded the <code>ArmForge</code> module first.</p> <pre><code>module load tools/ArmForge/20.0.3\nmap\n</code></pre> <p>If you have set up the X11-forwarding correctly, the MAP window should appear.</p> <p></p> Cannot open the MAP interface? <p>The MAP interface does not show up? Check the error message in your terminal:</p> <ul> <li>Error <code>bash: map: command not found</code> means that you did not load the <code>ArmForge</code> module.</li> <li>Error <code>map: cannot connect to X server</code> means that you did not enable X11-forwarding when connecting to the access node or compute node.</li> </ul> <p></p> <p>To load the performance profile of the motorbike simulation:</p> <ol> <li>Click on the \"Arm MAP\" tab on the left;</li> <li>Click on \"Load Profile Data File\" in the middle;</li> <li>Search for the profile data file <code>simpleFoam_XXX.map</code> that you have been generated.</li> </ol> <p>Once loaded, you should see a timeline with different metrics and many other panels. Check the online documentation to understand the role of the different panels. MAP follows the same color code as Performance Reports: the CPU metrics are in green; the MPI metrics are in blue; the memory metrics are in red; and the I/O metrics are in orange.</p> <p></p> <p>In our example, the source files are not visible in the middle panel because OpenFOAM has been compiled without the debugging information.</p> <p>Question 6: Exploring the metrics views in MAP</p> <p>In the MAP user interface,</p> <ul> <li>Select the metrics preset CPU Instructions:<ul> <li>What is the type of CPU instructions that occupy the CPU most of the time?</li> <li>Take a screenshot.</li> </ul> </li> <li>Select the metrics preset MPI:<ul> <li>What is the average amount of data sent by MPI over the whole simulation?</li> <li>If you select the first seconds of the simulation, how does this value change?</li> <li>Take a screenshot.</li> </ul> </li> <li>Select the metrics preset IO:<ul> <li>Can you identify how many times the OpenFOAM output is written to disk during the whole simulation?</li> <li>Does this match with the files present in the execution directory?</li> <li>Take a screenshot.</li> </ul> </li> </ul> <p>Add all the screenshots and your answers to your report.</p> <p>Question 7: Exploring the Thread Stacks in MAP</p> <p>In the bottom panel of the MAP user interface, select the tab Main Thread Stacks:</p> <ul> <li>In which function is done most of the MPI communication? (remember the color code)</li> <li>Save a screenshot in your report.</li> </ul> <p>Add the screenshot and your answer to the report.</p>"},{"location":"debugging_profiling/#part-3-debugging","title":"\ud83d\udd35 Part 3: Debugging","text":"<p>For the debugging part, we will use the Arm/Linaro DDT parallel debugger. Similarly to Performance Reports and MAP, it is a tool developed by Linaro (and previously Arm and Allinea).</p> DDT is a commercial tool <p>DDT is part of the Linaro Forge tool suite and was previously known as Arm Forge or Allinea Forge. It is a commercial tool that requires a licence to be used, and thus it is not available on all the HPC platforms.</p> Using DDT on Uni.lu HPC <p>The University of Luxembourg has purchased a license for Arm Forge (including MAP and DDT) and it is available to use for the user of the HPC platform. The version installed on the Uni.lu HPC is 20.0.3 which was released by Arm.</p> <p>On the Aion and Iris cluster, Arm DDT is available via the module <code>tools/ArmForge/20.0.3</code> and the command line executable is named <code>ddt</code>.</p>"},{"location":"debugging_profiling/#estimation-of-pi-using-the-monte-carlo-method","title":"Estimation of Pi using the Monte Carlo Method","text":"<p>For this part, we will work on a small MPI program that estimates the value of Pi using the Monte Carlo Method. In short, this approach generates random points and counts how many fall in the circle enclosed by the unit square.</p> <p>More detailed explanations can be found online:</p> <ul> <li>Estimating Pi using the Monte Carlo Method</li> <li>Monte Carlo algorithm</li> </ul> <p>An MPI program of this algorithm is available in the <code>debugging</code> directory of your GitHub Classroom repository. For this algorithm, the parallelization with MPI is extremely simple:  All the generated points are independent of each other, so each MPI process can work on a set of points separately without communication. When the total number of points has been reached, the results from all the MPI processes are collected and summed up on the main process (using <code>MPI_Reduce()</code>), which then can calculate the final estimation of Pi .</p> <p>Let's compile this program:</p> <ol> <li>Go to the <code>debugging</code> directory</li> <li>Load the FOSS toolchain</li> <li>Run <code>make</code> to compile the program</li> </ol> <p>Then, you can run the simulation with <code>mpirun -n &lt;nb proc&gt; ./pi_monte_carlo &lt;nb iterations&gt;</code>. For example</p> <pre><code>mpirun -n 1 ./pi_monte_carlo 100000000\nmpirun -n 2 ./pi_monte_carlo 100000000\nmpirun -n 4 ./pi_monte_carlo 100000000\nmpirun -n 8 ./pi_monte_carlo 100000000\n</code></pre>"},{"location":"debugging_profiling/#setup-the-debugger","title":"Setup the debugger","text":"<p>The Makefile comes with a suite of tests that can be executed with <code>make tests</code>. However, it appears there are some surprising results for some of the tests. There are probably a few bugs \ud83e\udeb2 left by a careless developer.</p> <p></p> <p>Let's chase the bugs with the Arm DDT debugger.</p> <p>First, compile with debugging information</p> <p>Before starting debugging, we need to recompile the program with the debugging information.  This will help the compiler to display more information about the program being executed.</p> <ol> <li>Open the <code>Makefile</code> file with your favorite text editor.</li> <li>Add the <code>-g</code> option to the <code>CFLAGS</code>, save and exit.</li> <li>Recompile the program with <code>make</code></li> </ol> <p>A simple way to use this compiler is to prefix the program command line with <code>ddt</code>:</p> <p><pre><code>module load tools/ArmForge/20.0.3\nddt mpirun -n  3 ./pi_monte_carlo\n</code></pre> and then just click the Run button.</p> <p></p> Cannot open the DDT interface? <p>The DDT interface does not show up? Check the error message in your terminal:</p> <ul> <li>Error <code>bash: ddt: command not found</code> means that you did not load the <code>ArmForge</code> module.</li> <li>Error <code>ddt: cannot connect to X server</code> means that you did not enable X11-forwarding when connecting to the access node or compute node.</li> </ul> <p>Check the online documentation to understand the DDT user interface.</p>"},{"location":"debugging_profiling/#find-the-bugs","title":"Find the bugs! \ud83d\udd0e","text":"<p>There are two known bugs in this MPI program. They are highlighted by the test suite executed by <code>make test</code>. </p> <ul> <li>Bug 1: The number of iterations used is shown to be 99999999 instead of 100000000. It can be reproduced with the command <code>mpirun -n 3 ./pi_monte_carlo 100000000</code>.</li> <li>Bug 2: The estimated value of Pi becomes negative and far from the expected value. It can be reproduced with the command <code>mpirun -n 16 ./pi_monte_carlo 10000000000</code>.</li> </ul> <p>Use the DDT debugger to track and exterminate these bugs. </p> <p>Tips to use DDT debugger</p> <ul> <li>The buttons on the toolbar control the execution of the program: for example, \"Step Over\" will execute only one line.</li> <li>Click on the left of a line number to set a breakpoint. The debugger will stop at this line before it is executed.</li> <li>In the right panel, the tab Locals shows the values of the local variables. Right-click on a variable and use Compare Across Processes to see the value of the different processes.</li> <li>Select \"File\" and \"Restart Session\" to restart the execution from the beginning.</li> </ul> <p>Debugging Methodology</p> <ul> <li>Identify the variables that show incorrect values in the output</li> <li>Identify at which lines these variables are modified</li> <li>Set breakpoint to check how the values are calculated (What is the expected value? What do you get?)</li> </ul> <p>Question 8: Debugging the Pi Monte Carlo program</p> <p>Find the two bugs in the Pi Monte Carlo program. For each bug,</p> <ul> <li>Explain the cause of the bug;</li> <li>Propose a correction;</li> <li>Show the results for the correct execution.</li> </ul> <p>Write down these answers in your report. And finally, add the corrected program to your GitHub repository.</p>"},{"location":"debugging_profiling/#report-submission","title":"\ud83d\udd35 Report submission","text":"<p>Don't forget to add and push all the required files to your GitHub Classroom repository:</p> <ul> <li>the submission scripts;</li> <li>the performance reports and profiles;</li> <li>the corrected program;</li> <li>the report, including the answers to all the questions and the screenshots.</li> </ul> <p>Don't add any other files and directories (for example, build directories, executables, Makefile, etc.). Finalize your report and save it as a PDF (no other format allowed!) in the <code>report</code> sub-directory of your GitHub Classroom repository.</p> Uploading your report <p>You don't need to copy it to the HPC to upload your report. Instead, you can use the Add file button directly on the GitHub webpage.</p> <p>Files Submission</p> <p>Your files and report are considered submitted once they are pushed to your repository.</p> <p>Check your GitHub repository online to make sure: https://github.com/MHPC-HPCSoftwareEnvironment/w03-debugging-and-profiling-XXX</p> <p>Passed the deadline at 11/03/2024 23h59 you won't be able to push anymore.</p>"},{"location":"debugging_profiling/#additional-resources","title":"\ud83d\udd35 Additional Resources","text":"<ul> <li>VI-HPC Tools Guide</li> </ul> <p>About Arm/Linaro Performance Reports:</p> <ul> <li>Linaro Performance Reports Homepage</li> <li>Linaro Performance Reports Documentation: Interpret performance reports</li> <li>Arm Performance Reports User Guide Version 20.0.3</li> <li>NERSC Documentation on Linaro Performance Reports</li> </ul> <p>About Arm/Linaro MAP:</p> <ul> <li>Linaro MAP Homepage</li> <li>Arm MAP User Guide Version 20.0.3</li> <li>NERSC Documentation on Linaro MAP</li> </ul> <p>About Arm/Linaro DDT:</p> <ul> <li>Linaro DDT Homepage</li> <li>Arm DDT User Guide Version 20.0.3</li> <li>NERSC Documentation on Linaro DDT</li> </ul>"},{"location":"software_installation/","title":"Software Installation on HPC","text":"<ul> <li>Author: Xavier Besseron (University of Luxembourg)</li> <li>License: \u00a92024 CC BY-NC-SA 4.0</li> <li>Date of practical: Tuesday 12th of March 2024</li> </ul>"},{"location":"software_installation/#introduction","title":"\ud83d\udd35 Introduction","text":"<p>Welcome to our practical session on installing scientific software on HPC. HPC environments are essential for large-scale simulations and computationally intensive research. Understanding how to install the necessary tools correctly is a vital skill for the efficient use of these powerful resources.</p>"},{"location":"software_installation/#objectives","title":"Objectives","text":"<p>During this session, we will equip you with hands-on experience in tackling the challenges of software installation on HPC:</p> <ul> <li>Modulefile Management: You'll learn how to manually create modulefiles, providing a structured way to manage different software versions and their dependencies.</li> <li>EasyBuild: We'll demonstrate EasyBuild, a powerful tool that streamlines software installations on HPC systems. You'll install software using existing recipes.</li> <li>Customizing EasyBuild: You'll delve into writing your own EasyBuild recipes. This gives you maximum flexibility when standard recipes aren't available.</li> <li>EESSI: We'll introduce the European Environment for Scientific Software Installations (EESSI). This project aims to simplify collaboration by providing standardized software stacks for HPC.</li> </ul> <p>The exercises will also refer to external documentation and instructions that need to be consulted. The aim is for you to be independent and work in 'real conditions' by finding the information you need to complete the tasks.</p>"},{"location":"software_installation/#instructions","title":"Instructions","text":"<p>This practical is an individual work to be carried out on the Aion cluster of the HPC platform of the University of Luxembourg.</p> <p>It is composed of different elements:</p> <ul> <li>this page that contains instructions and questions;</li> <li>the GitHub Classroom repository that contains the materials for the exercises;</li> <li>the report that you have to submit via GitHub Classroom with the rest of the requested documents</li> </ul>"},{"location":"software_installation/#report","title":"Report","text":"<p>Together with this tutorial, you need to prepare a short report of your work:</p> <ul> <li>There is no template, you can start from a blank document. Keep it simple, clear and well-organized.</li> <li>Make sure you answer all the questions (cf below), with text, code or screenshots as requested.</li> <li>Prepare your report at the same time you're doing the exercises.</li> <li>Submit your report as a PDF file to your GitHub Classroom repository and push it before the deadline.</li> </ul> <p>Question 1: What? Who? When?</p> <p>Indicate at the beginning of your report:</p> <ul> <li>the title of the practical</li> <li>your full name</li> <li>your Uni.lu HPC username</li> <li>your GitHub username</li> <li>the date</li> </ul>"},{"location":"software_installation/#part-0-setup","title":"\ud83d\udd35 Part 0: Setup","text":""},{"location":"software_installation/#github-classroom-repository","title":"GitHub Classroom Repository","text":"<p>Following the same approach as the previous practicals, this practical relies on Git Classroom to distribute the extra materials and for the submission of the report. To create your copy of the repository, follow the invitation link and accept the assignment:</p> <p>Assignment link: https://classroom.github.com/a/2BpAhQtZ</p> <p>If you need detailed explanations, please refer to the first week's instructions for Setting up Git and downloading the materials.</p>"},{"location":"software_installation/#connection-to-a-compute-node","title":"Connection to a Compute Node","text":"<p>The exercises are to be carried out in interactive mode on a compute node of the Aion cluster of the University of Luxembourg.</p> <p>As a reminder, here are the steps to access a compute nodes:</p> <ul> <li>Connect to the access node of the Aion cluster (cf Connection to HPC Access)</li> <li>Find your reservation, if application (cf Finding your reservation)</li> <li>Connect to a compute node (cf Connection to a compute node)</li> </ul> Recommended Settings <p>For this practical session, the following options are suggested:</p> <ul> <li>Partition: <code>interactive</code></li> <li>QoS: <code>debug</code></li> <li>Reservation: <code>hpc_software_d12</code> during the Tuesday afternoon session</li> <li>Time: 2 hours (or ending before the end of the reservation)</li> <li>Node: One compute node</li> <li>Tasks: One task</li> <li>Cores: 32 cores per task</li> </ul> <p>Your access command line should look like this:</p> <pre><code>salloc -p interactive --qos debug --reservation=hpc_software_d12 --time=2:00:00 -N 1 -n 1 -c 32\n</code></pre>"},{"location":"software_installation/#part-1-modulefile-for-manual-software-installation","title":"\ud83d\udd35 Part 1: Modulefile for Manual Software Installation","text":"<p>In this part, we want to create a modulefile for software installed manually. But first, let's have a look at the modulefiles already installed on the Aion cluster.</p>"},{"location":"software_installation/#inspecting-existing-modulefiles","title":"Inspecting existing Modulefiles","text":"<p>Numerous software are installed on the Aion cluster for the users.  They are installed in separate directories to avoid conflicting with each other.</p> <p>For example, we can find the modulefiles for the SCOTCH library using the instructions below. The path to the modulefile is shown at the top of the <code>module show</code> output, followed by the content of the modulefile.</p> <pre><code># Search for the SCOTCH module\nmodule avail SCOTCH\n\n# Show the content of the modulefile\nmodule show math/SCOTCH/6.1.0-gompi-2020b\n</code></pre> <p>Question 2: Inspecting the SCOTCH modulefile</p> <p>Answer all the questions below in your report:</p> <ul> <li>What is the full path to the SCOTCH modulefile?</li> <li>What is the homepage of the SCOTCH library, as given in the module?</li> <li>What is the installation path of the SCOTCH library?</li> </ul> <p>This version of SCOTCH has been installed with EasyBuild. </p> <ul> <li>Can you tell which version of EasyBuild has been used to install it? Where did you find this information?</li> <li>Can tell where all the modulefiles are installed?</li> <li>How many modulefiles are installed in this path? You can use <code>find PATH -type f -name '*.lua' | wc -l</code> to count all the <code>.lua</code> files in a given path.</li> </ul>"},{"location":"software_installation/#writing-your-own-modulefile","title":"Writing your own Modulefile","text":"<p>The objective of this exercise is to write your modulefile for software installed manually. We will use the preCICE library that you installed during Part 4 of the practical session on Compilation on HPC.</p> preCICE Installation <p>This exercise makes use of the preCICE installation that you have performed during the previous practical session. You will just need to use the full path to the preCICE installation directory. It should look like this:</p> <p></p> <p>If you did not have this installation, you will need to perform it again using the instructions of the practical.</p> <p>The Aion cluster uses the Lmod Environment Module system for which modulefiles are written in Lua,  and thus the modulefile should respect the Lua and Lmod syntax. The Lmod documentation provides many examples and a list of Lua functions that can help you to write a modulefile.</p> <ul> <li>An Introduction to Writing Modulefiles</li> <li>Modulefile Examples from simple to complex</li> <li>Functions for Lua Modulefile</li> <li>Module names and module naming conventions</li> </ul> <p>We will create our new modulefile in the <code>precice/modules</code> directory of the GitHub Classroom repository. We will follow the naming convention Category/Name/Version with</p> <ul> <li>Category = <code>numlib</code></li> <li>Name = <code>preCICE</code></li> <li>Version = <code>3.0.0</code></li> </ul> <p>and thus the preCICE modulefile will be named <code>numlib/preCICE/3.0.0.lua</code>. Here <code>numlib</code> and <code>preCICE</code> are sub-directories and <code>3.0.0.lua</code> is the actual filename)</p> <p>Task 1: Create an empty file for the new modulefile at the right location following the defined naming convention. You might need to create additional sub-directories.</p> <p>Task 2: Edit the new modulefile <code>numlib/preCICE/3.0.0.lua</code> to match your preCICE installation. Of course, you can get inspiration from the SCOTCH modulefile that you inspected earlier. You need to pay attention to the following items:</p> <ul> <li>You can get a short description from the preCICE homepage</li> <li>Your module should load the appropriate dependencies. They correspond to the modules you had to load to compile preCICE. Check your notes for Question 6 for the practical on  Compilation on HPC. NOTE: CMake is not needed as a dependency because it is only used for compilation but not during execution.</li> <li>Update these environment variables<ul> <li><code>PATH</code> for the directory <code>bin</code> containing the executables;</li> <li><code>CPATH</code> for the directory <code>include</code> containing the preCICE headers;</li> <li><code>LD_LIBRARY_PATH</code> and <code>LIBRARY_PATH</code> for the directory <code>lib64</code> containing the preCICE libraries;</li> <li><code>MANPATH</code> for the directory <code>share/man</code> containing the manpages;</li> <li><code>PKG_CONFIG_PATH</code> for the directory <code>lib/pkgconfig</code> containing the pkgconfig settings;</li> <li><code>CMAKE_PREFIX_PATH</code> for the preCICE installation directory as a CMake search path.</li> </ul> </li> </ul> <p>Question 3: Modulefile for preCICE installation</p> <p>Add your Modulefile for the preCICE installation to your GitHub Classroom repository, at the location defined above.</p>"},{"location":"software_installation/#test-your-modulefile","title":"Test your Modulefile","text":"<p>To use your new modulefile, we need to update the Lmod search path with <code>module use</code> (this will effectively update the <code>MODULEPATH</code> environment variable). After this, preCICE should be available in the list of modules</p> <pre><code># Update Lmod search path\nmodule use /PATH/TO/REPO/precice/modules\n\n# Look for the preCICE module\nmodule avail preCICE\n</code></pre> <p>You can now use and test the preCICE library. A small example is available in <code>precice/dummy_solver_cpp</code> for this purpose. Run the following instructions to try it:</p> <pre><code># Load preCICE module\nmodule load numlib/preCICE/3.0.0\n# Load the CMake module, needed by the example\nmodule load devel/CMake/3.20.1-GCCcore-10.2.0\n\n# Check the path to the preCICE executable\nwhich precice-tools\n# Check the preCICE version\nprecice-tools --version\n\n# Go to the example directory\ncd precice/dummy_solver_cpp\n\n# Configure the build\ncmake .\n# Build the example\nmake\n# Run the test\nctest -V\n</code></pre> <p>If your modulefile is correct, the CMake, Build and Test steps should be successful, as shown on the screenshot below.</p> <p></p> <p>Question 4: Testing the preCICE modulefile</p> <ul> <li>Run the commands above to test your preCICE modulefile</li> <li>Take a screenshot of your terminal output  \ud83d\udcf8</li> <li>Add the screenshot to your report</li> </ul>"},{"location":"software_installation/#part-2-software-installation-with-easybuild","title":"\ud83d\udd35 Part 2: Software Installation with EasyBuild","text":"<p>In this part, you will learn to use EasyBuild: </p> <ol> <li>Installation of EasyBuild itself</li> <li>Installation of software with EasyBuild</li> <li>Creation of EasyBuild recipes for other software</li> </ol>"},{"location":"software_installation/#easybuild-initial-configuration","title":"EasyBuild Initial Configuration","text":"<p>Before using EasyBuild, let's do some initial setup.</p> <p>EasyBuild is already available on the Aion cluster. Let's find the current version installed.</p> <ol> <li>Use <code>module</code> to find the currently available version of EasyBuild and load it</li> <li>Check the current version of EasyBuild with <code>eb --version</code> (the command to run EasyBuild is <code>eb</code>)</li> <li>Check the current EasyConfig configuration with <code>eb --show-config</code></li> </ol> <p>Question 5: EasyBuild initial setup</p> <p>Answer to these questions in your report:</p> <ul> <li>Take a screenshot \ud83d\udcf8 of the 3 steps above and add it to your report.</li> <li>What is the current version of EasyBuild available on Aion?</li> <li>What are the options with non-default values in the EasyBuild configuration?</li> <li>What is the installation path used by EasyBuild?</li> </ul> <p>Task 1: To make the software installed by EasyBuild searchable with <code>module</code>, update the Lmod search path accordingly:</p> <pre><code># Update Lmod search path\nmodule use \"${EASYBUILD_PREFIX}/modules/all\"\n</code></pre> <p>Task 2: To make this change permanent, add the following lines at the end of your <code>~/.bashrc</code> file:</p> <pre><code># Include the EasyBuild installation path to the Lmod search path\nif command -v module &gt;/dev/null 2&gt;&amp;1 ; then\n    module use \"${EASYBUILD_PREFIX}/modules/all\"\nfi\n</code></pre> What are these lines doing? <p>The <code>~/.bashrc</code> contains a list of commands that are executed every time a Bash shell is started.</p> <p>The <code>if command -v module &gt;/dev/null 2&gt;&amp;1</code> detects if the <code>module</code> command is available, in which case the Lmod search path is updated. If the <code>module</code> command is not available, it means that the shell is running on the access node instead of a compute node. Hence there is no need to update the module search path.</p>"},{"location":"software_installation/#installingupgrading-easybuild","title":"Installing/Upgrading EasyBuild","text":"<p>The installation procedure of EasyBuild is described on its documentation website. However, we can also leverage EasyBuild itself to install the latest version of EasyBuild.</p> <p>Following these steps to install the latest version of EasyBuild:</p> <ol> <li>Load the current version of EasyBuild</li> <li>Search the EasyBuild help for the option to install the latest known version of EasyBuild: use <code>eb --help</code> or search the online documentation.</li> <li>Install the latest known version of Easybuild with the right command line option</li> <li>Use <code>module</code> to search for the newly installed version of EasyBuild and load it</li> <li>Check the version of the new EasyBuild with <code>eb --version</code></li> </ol> <p>Question 6: Installing the latest EasyBuild</p> <p>Answer to these questions in your report:</p> <ul> <li>What is the command line to install the latest version of EasyBuild with EasyBuild?</li> <li>What is the version of the newly installed easybuild?</li> </ul>"},{"location":"software_installation/#installing-software-with-easybuild","title":"Installing Software with EasyBuild","text":"<p>We can now use the latest EasyBuild to install new software on the Aion cluster.  Because you don't have administrator permissions, the software is installed in your home directory: the created modules are only visible to you and the software can only be used by you.</p>"},{"location":"software_installation/#hpl-benchmark","title":"HPL Benchmark","text":"<p>We will start by installing HPL with EasyBuild.  HPL is a software package that solves a (random) dense linear system in double precision (64 bits) arithmetic on distributed-memory computers.  It can thus be regarded as a portable as well as freely available implementation of the High-Performance Computing Linpack Benchmark.</p> <p>If you remember well, you installed HPL manually at the end of the practical session on compilation.  Let's see if the installation is easier with EasyBuild.</p> <p>Perform the steps below to install HPL with EasyBuild:</p> <ol> <li>Search for HPL among available EasyConfigs: <code>eb --search HPL</code></li> <li>Search for HPL restricted to the <code>foss-2020b</code> toolchain: <code>eb --search HPL.*foss-2020b</code><ul> <li>The pattern <code>HLP.*foss-2020b</code> searches for files containing <code>HLP</code> and <code>foss-2020b</code> to see only the files based on the <code>foss/2020b</code> toolchain</li> </ul> </li> <li>Install HPL using the EasyConfig filename found: <code>eb HPL-XXX.eb</code> (no need to give the full path)</li> </ol> <p>EasyBuild will show the progress and the different steps of the compilation and installation. If the installation is successful, you should see the message below:</p> <pre><code>== COMPLETED: Installation ended successfully (took X secs)\n== Results of the build can be found in the log file(s) /XXX/HPL/2.3-foss-2020b/easybuild/easybuild-HPL-XXX.log\n</code></pre> <p>The log file <code>/XXX/HPL/2.3-foss-2020b/easybuild/easybuild-HPL-XXX.log</code> contains detailed information about the compilation and installation. It can be used to debug issues happening during the installation process.</p> <p>You can now test HPL with these lines:</p> <pre><code># Load the HPL module\nmodule load tools/HPL/2.3-foss-2020b\n# Copy the HPL input file in the current directory\ncp \"${EBROOTHPL}/bin/HPL.dat\" .\n# Run HPL on 4 cores\nsrun -n 4 -c 1 xhpl\n</code></pre> <p>Question 7: Installation of HPL</p> <p>Write the answer in your report:</p> <ul> <li>What is the full path of the EasyConfig file that you used to build HPL?</li> <li>What is the version of HPL built with this EasyConfig file?</li> <li>Add the log file generated by EasyBuild to the <code>hpl</code> directory of your GitHub Classroom repository.</li> <li>In your opinion, is it easier to install HPL manually or with EasyBuild? Why?</li> </ul>"},{"location":"software_installation/#osu-micro-benchmarks","title":"OSU Micro Benchmarks","text":"<p>We will now install the latest version of the OSU Micro Benchmarks.</p> <p>The OSU Micro-Benchmarks are a collection of programs designed  to measure the performance of various communication libraries, including MPI, OpenSHMEM, UPC++ and NCCL. </p> <p>These benchmarks measure the low-level performance of fundamental communication patterns:</p> <ul> <li>Point-to-point: Direct communication between two processes.</li> <li>Collective: Communication involving multiple processes (e.g., broadcast, reduce).</li> <li>One-sided: One process directly accessing the memory of another without explicit involvement from the remote process.</li> </ul> <p>Are OSU Micro Benchmarks available in EasyBuild?</p> <pre><code># Look for EasyConfig with a name containing 'OSU'\neb --search OSU\n# Look for OSU-Micro-Benchmarks with toolchain version 2020b\neb --search OSU-Micro-Benchmarks.*2020b\n# Look for OSU-Micro-Benchmarks with toolchain foss-2020b\neb --search OSU-Micro-Benchmarks.*foss-2020b\n</code></pre> WARNING: Found one or more non-allowed loaded (EasyBuild-generated) modules in current environment <p>EasyBuild can complain if some specific modules are loaded because it can affect the results of the build.   If such case, it shows the warning <code>WARNING: Found one or more non-allowed loaded (EasyBuild-generated) modules in current environment</code>.</p> <p>To get rid of this warning, clean your environment with <code>module unload ...</code>. Alternatively, you can run <code>module purge</code> and then load the EasyBuild module again.</p> <p></p> <p>According to the homepage, the latest version of the OSU Micro Benchmarks is version 7.3. Can we install them on Aion using the old toolchain <code>foss/2020b</code>?</p> <p>Let's use the advanced build options of EasyBuild:</p> Command line option Description <code>--try-software=NAME,VERSION</code> Try to search and build software with given name and version (type comma-separated list) <code>--try-software-name=NAME</code> Try to search and build software with given name <code>--try-software-version=VERSION</code> Try to search and build software with given version <code>--try-toolchain=NAME,VERSION</code> Try to search and build with given toolchain (name and version) (type comma-separated list) <code>--try-toolchain-name=NAME</code> Try to search and build with given toolchain name <code>--try-toolchain-version=VERSION</code> Try to search and build with given toolchain version <p>Task: Use EasyBuild with the right combination of command line options to install the OSU Micro Benchmarks version 7.3 with the toolchain <code>foss/2020b</code>.</p> <p>With the <code>--try-XXX</code> options, EasyBuild will generate a new EasyConfigs file in your current directory and try to build with it.</p> <p>Question 8: Installation of OSU Micro Benchmarks 7.3</p> <ul> <li>What is the EasyBuild command line to install the latest OSU Micro Benchmarks on Aion?</li> <li>Add the generated EasyConfig file to the <code>omb</code> directory of your GitHub Classroom repository.</li> <li>Add the EasyBuild log file  to the <code>omb</code> directory of your GitHub Classroom repository.</li> </ul>"},{"location":"software_installation/#creating-your-own-easyconfig","title":"Creating your own EasyConfig","text":"<p>For this exercise, we would like to install the MolSSI Driver Interface (MDI) library. It is a standardized API for fast, on-the-fly communication between computational chemistry codes. There is an EasyConfig recipe available  to build MDI version 1.4.16 with the toolchain <code>gompi/2022b</code>. However, we would like to build the most recent version 1.4.26 with the old toolchain <code>gompi/2020b</code> of Aion.</p> <p>Task 1: Adapt the EasyConfig file MDI-1.4.16-gompi-2022b.eb to build MDI 1.4.26 with toolchain gompi/2020b:</p> <ul> <li>Make sure to use the correct name for the new EasyConfig file;</li> <li>Update the software and toolchain version</li> <li>Adapt the dependencies to use the versions available on Aion (use <code>module avail</code>)</li> <li>Update the checksum of the source code archive (use <code>sha256sum MDI_Library-1.4.26.tar.gz</code> on the manually downloaded file to get the new checksum)</li> </ul> <p>Task 2: Build MDI 1.4.26 with your new EasyConfig file</p> <p>Question 9: EasyConfig file for MDI 1.4.26</p> <p>Add the following file to the <code>mdi</code> directory of your GitHub Classroom repository:</p> <ul> <li>Your new EasyConfig file for MDI 1.4.26</li> <li>The EasyBuild log file of the installation</li> </ul>"},{"location":"software_installation/#part-3-using-eessi","title":"\ud83d\udd35 Part 3: Using EESSI","text":"<p>EESSI or European Environment for Scientific Software Installations is installed on the Uni.lu HPC platform. The instructions to use it are detailed in the ULHPC Technical Documentation.</p> <p>In this part, we want to use EESSI on the Aion cluster to run the OpenFOAM motorbike simulation that we used in the previous practicals. In this situation, the advantage of EESSI is that it provides a more recent version of OpenFOAM.</p> <p>Task 1: Following the online documentation, set up the EESSI environment and the OpenFOAM version that is provided in EESSI.</p> <p>Question 10: Exploring the EESSI environment</p> <p>Take a screenshot \ud83d\udcf8 of your terminal output when setting up the EESSI environment and add it to your report.</p> <p>From the output of your commands, can you tell</p> <ul> <li>What is the processor architecture detected by EESSI?</li> <li>What is the path to the modulefiles provided by EESSI?</li> <li>What is the name of the OpenFOAM module provided by EESSI?</li> </ul> <p>Task 2: Run the OpenFOAM motorbike simulation using the OpenFOAM provided by EESSI:</p> <ul> <li>Copy the input directory <code>motorBike-INPUT</code> (located in <code>eessi/</code>) to a new directory called <code>motorBike-RUN-32</code></li> <li>Adapt the submission script <code>run_OpenFOAM_parallel.sh</code><ul> <li>Set the number of cores to 32 (i.e., 32 tasks with 1 CPU per task)</li> <li>Add the command to set up the EESSI environment</li> <li>Set the working directory</li> <li>Load the OpenFOAM module from EESSI</li> </ul> </li> <li>Submit the script with <code>sbatch</code></li> <li>Monitor the execution with <code>sq</code> and <code>tail -f SLURM_motorbike_XXX.log</code></li> <li>Check the execution log file <code>SLURM_motorbike_XXX.log</code> for successful execution</li> </ul> <p>Question 10: Using OpenFOAM provided by EESSI</p> <p>Add the following files to the <code>eessi</code> directory of your GitHub Classroom repository:</p> <ul> <li>Your modified <code>run_OpenFOAM_parallel.sh</code> script</li> <li>Your execution log file  <code>SLURM_motorbike_XXX.log</code></li> </ul>"},{"location":"software_installation/#report-submission","title":"\ud83d\udd35 Report submission","text":"<p>Don't forget to add and push all the required files to your GitHub Classroom repository:</p> <ul> <li>the log files, the EasyConfig files and the submission scripts;</li> <li>the report, including the answers to all the questions and the screenshots.</li> </ul> <p>Don't add any other files and directories (for example, build directories, executables, Makefile, etc.). Finalize your report and save it as a PDF (no other format allowed!) in the <code>report</code> sub-directory of your GitHub Classroom repository.</p> Uploading your report <p>You don't need to copy it to the HPC to upload your report. Instead, you can use the Add file button directly on the GitHub webpage.</p> <p>Files Submission</p> <p>Your files and report are considered submitted once they are pushed to your repository.</p> <p>Check your GitHub repository online to make sure: https://github.com/MHPC-HPCSoftwareEnvironment/w04-software-installation-XXX</p> <p>Passed the deadline at 18/03/2024 23h59 you won't be able to push anymore.</p>"},{"location":"software_installation/#additional-resources","title":"\ud83d\udd35 Additional Resources","text":""},{"location":"software_installation/#about-lmod-environment-module","title":"About Lmod Environment Module","text":"<ul> <li>Lmod Documentation</li> <li>An Introduction to Writing Modulefiles</li> <li>Modulefile Examples from simple to complex</li> <li>Functions for Lua Modulefile</li> <li>Module names and module naming conventions</li> </ul>"},{"location":"software_installation/#about-easybuild","title":"About EasyBuild","text":"<ul> <li>EasyBuild Homepage</li> <li>EasyBuild Documentation</li> <li>Command Line / Configuration Options</li> <li>Searching for EasyConfig files</li> <li>Common toolchains</li> <li>Writing easyconfig files</li> <li>EasyBuild Tutorial at ISC'22</li> </ul>"},{"location":"software_installation/#about-eessi","title":"About EESSI","text":"<ul> <li>EESSI Homepage</li> <li>Using EESSI on Uni.lu HPC</li> </ul>"},{"location":"testing/","title":"Testing and Continuous Integration in HPC","text":"<ul> <li>Author: Georgios Kafanas (University of Luxembourg), Xavier Besseron (University of Luxembourg)</li> <li>License: \u00a92024 CC BY-NC-SA 4.0</li> <li>Date of practical: Tuesday 19th of March 2024</li> </ul>"},{"location":"testing/#introduction","title":"\ud83d\udd35 Introduction","text":"<p>Welcome to our practical session on the testing of scientific software on HPC. HPC environments are essential for large-scale simulations and computationally intensive research. Understanding how to install the necessary tools correctly is a vital skill for the efficient use of these powerful resources.</p> <p>In this practical session, we're diving into the world of testing within HPC environments. HPC systems bring incredible power, but also unique challenges when ensuring our software and simulations are reliable and accurate. When dealing with critical calculations, from scientific breakthroughs to engineering designs, errors here can also have costly consequences.  That's why robust testing strategies tailored to HPC are essential.</p>"},{"location":"testing/#objectives","title":"Objectives","text":"<p>This practical will focus on the ReFrame testing framework.  ReFrame is a powerful tool for writing system regression tests and benchmarks, designed to run tests on HPC systems, but also supporting a wide range of platforms.  The goal of the framework is to separate the logic of the tests from the configuration required to run the tests in each HPC cluster.  Therefore, a single set of tests can be easily executed in a set of clusters by describing a configuration for each cluster.</p> <p>In this session, we will cover the execution and analysis of</p> <ul> <li>sanity and performance tests,</li> <li>that run either locally or on an HPC cluster component,</li> <li>on programs that are either compiled from source or installed using a build automation framework, such as EasyBuild.</li> </ul> <p>The exercises will also refer to external documentation and instructions that need to be consulted.  The aim is for you to be independent and work in 'real conditions' by finding the information you need to complete the tasks.</p>"},{"location":"testing/#instructions","title":"Instructions","text":"<p>This practical is an individual work to be carried out on the Aion cluster of the HPC platform of the University of Luxembourg.</p> <p>It is composed of different elements:</p> <ul> <li>this page that contains instructions and questions;</li> <li>the GitHub Classroom repository that contains the materials for the exercises;</li> <li>the report that you have to submit via GitHub Classroom with the rest of the requested documents</li> </ul>"},{"location":"testing/#report","title":"Report","text":"<p>Together with this tutorial, you need to prepare a short report of your work:</p> <ul> <li>There is no template, you can start from a blank document. Keep it simple, clear and well-organized.</li> <li>Make sure you answer all the questions (cf below), with text, code or screenshots as requested.</li> <li>Prepare your report at the same time you're doing the exercises.</li> <li>Submit your report as a PDF file to your GitHub Classroom repository and push it before the deadline.</li> </ul> <p>Question 0: What? Who? When?</p> <p>Indicate at the beginning of your report:</p> <ul> <li>the title of the practical</li> <li>your full name</li> <li>your Uni.lu HPC username</li> <li>your GitHub username</li> <li>the date</li> </ul>"},{"location":"testing/#part-0-setup","title":"\ud83d\udd35 Part 0: Setup","text":""},{"location":"testing/#github-classroom-repository","title":"GitHub Classroom Repository","text":"<p>Following the same approach as the previous practicals, this practical relies on Git Classroom to distribute the extra materials and for the submission of the report. To create your copy of the repository, follow the invitation link and accept the assignment:</p> <p>Assignment link: https://classroom.github.com/a/AeZ54-FZ</p> <p>If you need detailed explanations, please refer to the first week's instructions for Setting up Git and downloading the materials.</p>"},{"location":"testing/#connection-to-a-compute-node","title":"Connection to a Compute Node","text":"<p>The exercises are to be carried out in interactive mode on a compute node of the Aion cluster of the University of Luxembourg.</p> <p>As a reminder, here are the steps to access a compute nodes:</p> <ul> <li>Connect to the access node of the Aion cluster (cf Connection to HPC Access)</li> <li>Find your reservation, if application (cf Finding your reservation)</li> <li>Connect to a compute node (cf Connection to a compute node)</li> </ul> Recommended Settings <p>For this practical session, the following options are suggested:</p> <ul> <li>Partition: <code>interactive</code></li> <li>QoS: <code>debug</code></li> <li>Reservation: <code>hpc_software_d19</code> during the Tuesday afternoon session</li> <li>Time: 2 hours (or ending before the end of the reservation)</li> <li>Node: One compute node</li> <li>Tasks: One task</li> <li>Cores: 32 cores per task</li> </ul> <p>Your access command line should look like this:</p> <pre><code>salloc -p interactive --qos debug --reservation=hpc_software_d19 --time=2:00:00 -N 1 -n 1 -c 32\n</code></pre>"},{"location":"testing/#part-1-getting-started-with-reframe","title":"\ud83d\udd35 Part 1: Getting Started with ReFrame","text":""},{"location":"testing/#how-is-reframe-used-in-wider-cicd-systems","title":"How is ReFrame used in wider CI/CD systems?","text":"<p>ReFrame is one of the components of automated CI/CD systems that is particularly important in HPC. The ReFrame framework provides a simple API that can be used by automated CI/CD systems, such as Jenkins to launch tests. Tests can be triggered</p> <ul> <li>at specific events during software development, such push of new commits in a development branch, or before a new release;</li> <li>at specific events during the management of a cluster, such as the update of software or installation of new hardware;</li> <li>at regular time intervals as a catch-all measure.</li> </ul> <p>ReFrame does not control the launching of tests, but it will make sure that each test runs on all relevant systems. The figure below summarizes the set of logical decisions that ReFrame uses for each test specification to determine if a test has run for a given system and environment.</p> Figure 1: The regression test loop in ReFrame <p>Of course, ReFrame tests can also be launched manually as well. Even with automation, there are tests you should run always manually: You don't want to run cluster-wide stress tests without supervision!</p>"},{"location":"testing/#installation","title":"Installation","text":"<p>ReFrame is available in our cluster as a module. However, for instructive purposes, install and configure the latest module for ReFrame from scratch, as someone would in a new system.</p> Install the latest EasyBuild <p>If you have not done so during the last practical session, you need to install the latest EasyBuild first. You can follow the instructions of the previous practical session on Software Installation.</p> <pre><code># Load the current EasyBuild\nmodule load tools/EasyBuild/4.5.4\n# Install the latest EasyBuild\neb --install-latest-eb-release\n</code></pre> Set up your module search path <p>To find the software and modules installed in your home directory, you need to update your module search path <code>MODULEPATH</code> If you have not done it during the last practical sessions, you can follow the instructions of the previous practical session on Software Installation.</p> <p>To make the software installed by EasyBuild searchable with module in the current shell, run this command:</p> <pre><code># Update Lmod search path\nmodule use \"${EASYBUILD_PREFIX}/modules/all\"\n</code></pre> <p>To make the software installed by EasyBuild searchable with module permanently (recommended), add the following lines at the end of your <code>~/.bashrc</code> file:</p> <pre><code># Include the EasyBuild installation path to the Lmod search path\nif command -v module &gt;/dev/null 2&gt;&amp;1 ; then\n  module use \"${EASYBUILD_PREFIX}/modules/all\"\nfi\n</code></pre> <p>A simple way to install ReFrame is using EasyBuild. Search with EasyBuild for the readily available ReFrame easyconfigs. <pre><code># Use the latest EasyBuild\n$ module load tools/EasyBuild/4.9.0\n# Search for ReFrame recipe\n$ eb -S reframe\n== found valid index for /home/users/username/environments/easybuild/easybuild/easyconfigs, so using it...\nCFGS1=/home/users/username/environments/easybuild/easybuild/easyconfigs/r/ReFrame\n * $CFGS1/ReFrame-2.18.eb\n * $CFGS1/ReFrame-2.19.eb\n * $CFGS1/ReFrame-2.20.eb\n * $CFGS1/ReFrame-2.21.eb\n * $CFGS1/ReFrame-3.0.eb\n * $CFGS1/ReFrame-3.2.eb\n * $CFGS1/ReFrame-3.3.eb\n * $CFGS1/ReFrame-3.4.1.eb\n * $CFGS1/ReFrame-3.5.0.eb\n * $CFGS1/ReFrame-3.5.1.eb\n * $CFGS1/ReFrame-3.5.2.eb\n * $CFGS1/ReFrame-3.6.2.eb\n * $CFGS1/ReFrame-3.6.3.eb\n * $CFGS1/ReFrame-3.7.3.eb\n * $CFGS1/ReFrame-3.8.0.eb\n * $CFGS1/ReFrame-3.9.0.eb\n * $CFGS1/ReFrame-3.9.1.eb\n * $CFGS1/ReFrame-3.10.1.eb\n * $CFGS1/ReFrame-3.11.0.eb\n * $CFGS1/ReFrame-3.11.1.eb\n * $CFGS1/ReFrame-3.11.2.eb\n * $CFGS1/ReFrame-3.12.0.eb\n * $CFGS1/ReFrame-4.0.1.eb\n * $CFGS1/ReFrame-4.0.5.eb\n * $CFGS1/ReFrame-4.2.0.eb\n * $CFGS1/ReFrame-4.3.2.eb\n * $CFGS1/ReFrame-4.3.3.eb\n</code></pre> There are multiple recipes available. Install the latest available version. <pre><code>$ eb ReFrame-4.3.3.eb\n</code></pre></p> <p>After the installation, ReFrame should be available as a module at the cluster nodes. Start a job at a compute node, add the directory where your EasyBuild configuration installed the module files to the module search path, and you should be able to load ReFrame with the command <pre><code>module load devel/ReFrame/4.3.3\n</code></pre> as with any other software.</p>"},{"location":"testing/#part-2-running-tests","title":"\ud83d\udd35 Part 2: Running tests","text":"<p>Quite often you want to test software which you compile from source, for instance, because you are a developer on some project. In this section, we start with simple test programs compiled directly from source, and create sanity and performance tests. We then port our tests in the UL HPC Aion cluster.</p>"},{"location":"testing/#getting-access-to-the-tutorial-source-code","title":"Getting access to the tutorial source code","text":"<p>The tutorial follows closely the official ReFrame tutorials. The source code for the official tutorials is included in the source distribution of ReFrame. We have made some modifications to the source code so that it runs in our Aion cluster. You can find the modified tutorials in the assignment repository in the <code>tutorials</code> directory.</p> <p>Tutorial files</p> <p>All subsequent filenames are referenced relative to the <code>tutorials</code> directory of your GitHub Classroom repository.</p>"},{"location":"testing/#local-sanity-test-for-a-program-compiled-from-source","title":"Local sanity test for a program compiled from source","text":"<p>As a first example we will test with ReFrame that a \"Hello, World!\" program produces the desired output. Tests in ReFrame are organized in directories. The test directory is <code>basics/hello</code>. Printing out the test directory structure, <pre><code>$ tree basics/hello\nbasics/hello\n\u251c\u2500\u2500 hello1.py\n\u251c\u2500\u2500 hello2.py\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 hello.c\n    \u2514\u2500\u2500 hello.cpp\n\n1 directory, 4 files\n</code></pre> there is a subdirectory, <code>src</code>, with the source code, and 2 Python files.</p> <ul> <li>The <code>src</code> directory is where ReFrame will look for source code files by default. You can customize the source code directory name.</li> <li>The Python files contain our test specifications. ReFrame is a Python framework, meaning that our tests simply modify the behavior of the Python scripts running the tests. We can use all the features of Python language when specifying our tests.</li> </ul> <p>Let's have a look at the first test file, <code>basics/hello/hello1.py</code>. <pre><code>import reframe as rfm\nimport reframe.utility.sanity as sn\n\n@rfm.simple_test\nclass HelloTest(rfm.RegressionTest):\n    valid_systems = ['*']\n    valid_prog_environs = ['*']\n    sourcepath = 'hello.c'\n\n    @sanity_function\n    def assert_hello(self):\n        return sn.assert_found(r'Hello, World\\!', self.stdout)\n</code></pre></p>"},{"location":"testing/#the-contents-of-the-test-files","title":"The contents of the test files","text":"<p>This test concerns a single source file, <code>hello.c</code>, defined in the <code>sourcepath</code> option. All source paths provided are relative to the <code>src</code> directory by default. In fact, there are multiple test options that do not appear here because they use their default values. You can find details on how to customize the build options in the Build Systems section of the API. A few source code options that are often useful are</p> <ul> <li><code>srcdir</code>, which defines a custom source directory, and</li> <li><code>build_system</code>, which determines the type of build system used to compile the code.</li> </ul> <p>The <code>build_system</code> can determine that a single source file is compiled (<code>SingleSource</code>), that some build automation system is used (such as Make, CMake, and Autotools) or that some source code package manager such as EasyBuild and Spack is used. The value of <code>build_system</code> is often determined automatically from the contents of <code>src</code>. However, on some occasions, like when both Make and CMake compilation scripts are available, you may have to specify the build system. In our case where we compile single files, the <code>'SingleSource'</code> option, which is the default, is used.</p> <p>We set 2 more variables explicitly in our test file.</p> <ul> <li><code>valid_prog_environs</code>: sets valid programming environments. These are a collection of compilation options, including the compiler for each language, compilation flags, and procedural options, such as the number of cores used for the compilation and whether we compile locally or submit a job at the cluster.</li> <li><code>valid_systems</code>: sets the systems for which our test is valid. For instance, in our case, it can be a single Aion node or the whole Aion cluster.</li> </ul> <p>The environment and system components are specified in configuration files. If we do not provide a configuration file, then ReFrame will use default options. Assigning the <code>'*'</code> argument for some component means that our test will be executed for all instances specified for that component in our configuration files.</p> <p>Test options configuration</p> <p>All configuration options are defined in Python dictionaries in configuration Python files. Options are set in test files by assigning the appropriate field of the test class to a list of dictionary keys.</p> <p>The test file defines a single sanity check on the output of the compiled program. The test is defined in a class, <code>HelloTest</code>, which inherits from <code>reframe.RegressionTest</code> class and decorated with the <code>@rfm.simple_test</code> decorator. The <code>rfm.RegressionTest</code> class of tests analyzes the output of the compiled executable and the <code>@rfm.simple_test</code> registers the <code>HelloTest</code> as a test. There are types of tests for checking various kinds of output streams, such as compilation warnings and errors. The test defines a single sanity check function, <code>assert_hello</code>, which looks for the existence of the \"Hello, World!\" string in the standard output (<code>stdout</code>) of our program. The decorator <code>@rfm.simple_test</code> registers the <code>assert_hello</code> function as a function which will provide a test.</p> <ul> <li>The ReFrame framework makes extensive use of decorators to define tests and other actions.</li> <li>The special class of lazily evaluated function, the Deferrable Functions, is defined in <code>reframe.utility.sanity</code>. These functions are used to provide expression which will be evaluated on the output of programs when running the tests. The <code>assert_found</code> used in our sanity check is an instance of this class of functions.</li> </ul> <p>Composing deferrable functions</p> <p>You don't need to know the details of how deferrable functions work, just that they specify the computation of an expression that will be evaluated later when tests are run. You will need to convert your tests to expressions using deferrable functions.</p> <p>The documentation of the deferrable functions module (<code>reframe.utility.sanity</code>) should provide all the information you need.</p>"},{"location":"testing/#running-the-tests-and-examining-the-output","title":"Running the tests and examining the output","text":"<p>Run the tests by issuing a command to ReFrame with the path to the file with the tests (<code>--checkpath</code>) and the command to run the test (<code>--run</code>) as arguments. <pre><code>$ reframe --checkpath basics/hello/hello1.py --run\n[ReFrame Setup]\n  version:           4.3.3\n  command:           '/home/users/gkafanas/.local/easybuild/aion/2020b/epyc/software/ReFrame/4.3.3/bin/reframe --checkpath basics/hello/hello1.py --run'\n  launched by:       gkafanas@aion-0096\n  working directory: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials'\n  settings files:    '&lt;builtin&gt;'\n  check search path: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/basics/hello/hello1.py'\n  stage directory:   '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/stage'\n  output directory:  '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/output'\n  log files:         '/tmp/rfm-0ldxr9w8.log'\n\n[==========] Running 1 check(s)\n[==========] Started on Thu Mar  7 14:42:37 2024\n\n[----------] start processing checks\n[ RUN      ] HelloTest /2b3e4546 @generic:default+builtin\n[       OK ] (1/1) HelloTest /2b3e4546 @generic:default+builtin\n[----------] all spawned checks have finished\n\n[  PASSED  ] Ran 1/1 test case(s) from 1 check(s) (0 failure(s), 0 skipped, 0 aborted)\n[==========] Finished on Thu Mar  7 14:42:37 2024\nLog file(s) saved in '/tmp/rfm-0ldxr9w8.log'\n</code></pre></p> <p>Observe the output. ReFrame has run the test and reports</p> <ul> <li>the stage directory where the test where run,</li> <li>the output directory where the test output was stored,</li> <li>the check search path which is the path to the file containing the tests that were run, and</li> <li>that a detailed log of all the operations performed is saved in a temporary log file.</li> </ul> <p>You can explore these different paths and files to understand better how ReFrame works.</p> <p>Question 1: Locating test output artifacts</p> <p>Take a screenshot of the ReFrame output \ud83d\udcf8 and include it in your report.</p> <p>In your report, write down the following information:</p> <ul> <li>the path to the stage directory,</li> <li>the path to the output directory,</li> <li>the path to the file from which the tests were selected and run, and</li> <li>the path to the log file.</li> </ul>"},{"location":"testing/#examining-reframe-output","title":"Examining ReFrame output","text":"<p>The regression tests are executed in a few distinct phases summarized in Figure 2.</p> Figure 2: The regression test pipeline in ReFrame <p>We can discern a few of these steps in the output of ReFrame. The report mentions each test run, for instance <pre><code>[ RUN      ] HelloTest /2b3e4546 @generic:default+builtin\n</code></pre> and a summary of the test results, for instance: <pre><code>[       OK ] (1/1) HelloTest /2b3e4546 @generic:default+builtin\n</code></pre> Each report line begins with the test name (e.g. <code>HelloTest</code>) together with a unique hash for the test configuration (e.g. <code>2b3e4546</code>). The second part of the test description is: <pre><code>@generic:default+builtin\n</code></pre> This string describes the configuration of the system where the test is compiled and run. The general form of the system configuration string is <pre><code>@system:partition+environment\n</code></pre> where</p> <ul> <li><code>system</code> is a name identifying the configuration where our test was compiled and run, a system can contain multiple partitions and configurations,</li> <li><code>partition</code> is a description of how the test was run, in HPC systems this description can include options like the scheduler partition where the tests will be launched (hence the name) and required modules, and</li> <li><code>environment</code> is a description of how the test was compiled, including compilation flags and runtime options like the number of threads available to the compiler.</li> </ul> <p>Compilation location</p> <p>ReFrame compiles the source code locally, on the system where ReFrame is launched. The <code>partition</code> setting applies only to the tests. In an HPC system with a scheduler, you can thus submit multiple tests for the same executable to be run in parallel. If you want to compile and test multiple programs in parallel you have to launch multiple ReFrame jobs.</p> <p>The system configuration options are defined in configuration files (introduced later). The simple hello test uses the default options since no configuration file is specified when running ReFrame.</p> <p>Question 2: Locating the test pipeline phases in the log file</p> <p>The temporary log file contains detailed output for the test phase. Attach a copy of your log file to your report repository, and mention in your report in which line each test phase starts.</p>"},{"location":"testing/#examining-test-artifacts","title":"Examining test artifacts","text":"<p>The execution of the tests generates a few artifacts. These artifacts are stored in the <code>output</code> and <code>stage</code> directories created in the directory where ReFrame is launched. If you examine the output directory, it should contain the subdirectory <pre><code>output/generic/default/builtin/HelloTest\n</code></pre> where the various directory names in the path also appear in the test configuration string. This directory contains the following files: <pre><code>$ ls output/generic/default/builtin/HelloTest\nrfm_build.err  rfm_build.out  rfm_build.sh  rfm_job.err  rfm_job.out  rfm_job.sh\n</code></pre></p> <p>Let's have a look at the contents of the output files:</p> <ul> <li><code>rfm_build.sh</code> is a Bash script that compiles the test executable <pre><code>#!/bin/bash\n\n_onerror()\n{\n    exitcode=$?\n    echo \"-reframe: command \\`$BASH_COMMAND' failed (exit code: $exitcode)\"\n    exit $exitcode\n}\n\ntrap _onerror ERR\n\ncc hello.c -o ./HelloTest\n</code></pre></li> <li><code>rfm_build.out</code> and <code>rfm_build.err</code> contain the standard output and error output of the test compilation.</li> <li><code>rfm_job.sh</code> is a Bash script that runs the test executable. <pre><code>#!/bin/bash\n./HelloTest\n</code></pre></li> <li><code>rfm_job.out</code> and <code>rfm_job.err</code> contain the standard output and error output of the test execution.</li> </ul> <p>In this case, the contents of <code>rfm_job.out</code> are of particular interest, since they are analyzed by our sanity check. It contains a single line with the string <code>'Hello, World!'</code>. Our regression test defined with <code>sn.assert_found(r'Hello, World\\!', self.stdout)</code> simply checks for the existence of the string <code>'Hello, World!'</code> in the output file, so it will succeed.</p> <p>If later a change is introduced to our program and it produces the output <pre><code>Hello, World!\n123\n</code></pre> the regression test will still succeed! Of course, it is possible to use more sophisticated checks to catch such regressions, and they will be presented in the next sections.</p> <p>The contents of the <code>stage</code> directory have the same subdirectory structure as the <code>output</code> directory, but for a successful test, it will be empty. The stage directory is used to compile and run the code. During the test, the contents of the test directory <code>basics/hello</code> are transferred to the staging directory, and the code is compiled and executed. After a successful test, the contents of the staging directory are cleared. However, after a failed test the contents of the staging directory are retained so that you can diagnose the cause of the failure.</p> <p>Keeping the test output</p> <p>You can change the default option of deleting the staged files upon successful completion of a test with the flag <code>--keep-staged-files</code>.</p> <p>Question 3: Locating test artifacts</p> <p>Change the \"Hello, World!\" example to print to standard error instead of standard output. This is accomplished by replacing <pre><code>printf(\"Hello, World!\\n\");\n</code></pre> with: <pre><code>fprintf(stderr, \"Hello, World!\\n\");\n</code></pre> What are the contents of the output and stage directories now? Note down the list of files in each directory in your report.</p>"},{"location":"testing/#test-reports","title":"Test reports","text":"<p>ReFrame also saves the setup and output of each test as a test report in JSON format in the directory <code>~/.reframe/reports/</code>. Running <code>tree</code> after executing a single test, we get:</p> <pre><code>$ tree ~/.reframe/reports\n/home/users/gkafanas/.reframe/reports\n\u251c\u2500\u2500 latest.json -&gt; run-report-0.json\n\u2514\u2500\u2500 run-report-0.json\n\n0 directories, 2 files\n</code></pre> <p>We see a list of all the run reports and a symbolic link to the latest report. Not that ReFrame will not clean the report directory <code>~/.reframe/reports</code> for you!</p> <p>Redirecting the report output</p> <p>If you want ReFrame to print the report in a specific file use the flag <code>--report-file &lt;FILE&gt;</code>.</p> <p>The report files contain all the information printed in the <code>stdout</code> by ReFrame, but also a lot more detail. In particular, any default options are listed explicitly. You can use the report files to browse the options available for your test configurations.</p> Example report file <p>For instance, the report for the \"Hello, World!\" test contains the following: <pre><code>{\n  \"session_info\": {\n    \"cmdline\": \"/home/users/gkafanas/.local/easybuild/aion/2020b/epyc/software/ReFrame/4.3.3/bin/reframe --checkpath basics/hello/hello1.py --run\",\n    \"config_files\": [\n      \"&lt;builtin&gt;\"\n    ],\n    \"data_version\": \"3.1\",\n    \"hostname\": \"aion-0096\",\n    \"log_files\": [\n      \"/tmp/rfm-98ae7dcg.log\"\n    ],\n    \"prefix_output\": \"/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/output\",\n    \"prefix_stage\": \"/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/stage\",\n    \"user\": \"gkafanas\",\n    \"version\": \"4.3.3\",\n    \"workdir\": \"/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials\",\n    \"time_start\": \"2024-03-07T14:55:45+0100\",\n    \"time_end\": \"2024-03-07T14:55:46+0100\",\n    \"time_elapsed\": 0.5394518375396729,\n    \"num_cases\": 1,\n    \"num_failures\": 0\n  },\n  \"runs\": [\n    {\n      \"num_cases\": 1,\n      \"num_failures\": 0,\n      \"num_aborted\": 0,\n      \"num_skipped\": 0,\n      \"runid\": 0,\n      \"testcases\": [\n        {\n          \"build_stderr\": \"rfm_build.err\",\n          \"build_stdout\": \"rfm_build.out\",\n          \"dependencies_actual\": [],\n          \"dependencies_conceptual\": [],\n          \"description\": \"\",\n          \"display_name\": \"HelloTest\",\n          \"environment\": \"builtin\",\n          \"fail_phase\": null,\n          \"fail_reason\": null,\n          \"filename\": \"/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/basics/hello/hello1.py\",\n          \"fixture\": false,\n          \"hash\": \"2b3e4546\",\n          \"jobid\": \"4124802\",\n          \"job_stderr\": \"rfm_job.err\",\n          \"job_stdout\": \"rfm_job.out\",\n          \"maintainers\": [],\n          \"name\": \"HelloTest\",\n          \"nodelist\": [\n            \"aion-0096\"\n          ],\n          \"outputdir\": \"/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/output/generic/default/builtin/HelloTest\",\n          \"perfvars\": null,\n          \"prefix\": \"/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/basics/hello\",\n          \"result\": \"success\",\n          \"stagedir\": \"/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/stage/generic/default/builtin/HelloTest\",\n          \"scheduler\": \"local\",\n          \"system\": \"generic:default\",\n          \"tags\": [],\n          \"time_compile\": 0.1687169075012207,\n          \"time_performance\": 0.00466609001159668,\n          \"time_run\": 0.16573643684387207,\n          \"time_sanity\": 0.0048558712005615234,\n          \"time_setup\": 0.05423617362976074,\n          \"time_total\": 0.49439144134521484,\n          \"unique_name\": \"HelloTest\",\n          \"check_vars\": {\n            \"valid_prog_environs\": [\n              \"*\"\n            ],\n            \"valid_systems\": [\n              \"*\"\n            ],\n            \"descr\": \"\",\n            \"sourcepath\": \"hello.c\",\n            \"sourcesdir\": \"src\",\n            \"prebuild_cmds\": [],\n            \"postbuild_cmds\": [],\n            \"executable\": \"./HelloTest\",\n            \"executable_opts\": [],\n            \"prerun_cmds\": [],\n            \"postrun_cmds\": [],\n            \"keep_files\": [],\n            \"readonly_files\": [],\n            \"tags\": [],\n            \"maintainers\": [],\n            \"strict_check\": true,\n            \"num_tasks\": 1,\n            \"num_tasks_per_node\": null,\n            \"num_gpus_per_node\": null,\n            \"num_cpus_per_task\": null,\n            \"num_tasks_per_core\": null,\n            \"num_tasks_per_socket\": null,\n            \"use_multithreading\": null,\n            \"max_pending_time\": null,\n            \"exclusive_access\": false,\n            \"local\": false,\n            \"modules\": [],\n            \"env_vars\": {},\n            \"variables\": {},\n            \"time_limit\": null,\n            \"build_time_limit\": null,\n            \"extra_resources\": {},\n            \"build_locally\": true\n          },\n          \"check_params\": {}\n        }\n      ]\n    }\n  ],\n  \"restored_cases\": []\n}\n</code></pre></p> <p>We can find the options for</p> <ul> <li><code>valid_prog_environs</code>,</li> <li><code>valid_systems</code>,</li> <li><code>sourcepath</code>,</li> </ul> <p>and many more options and output results.</p> <p>Question 4: Analysing report files</p> <p>The ReFrame report file contains all the setup and output information for a test.</p> <p>Run the original \"Hello, World!\" test again, locate the ReFrame report and add it to your GitHub Classroom repository. In the report file note down the lines containing</p> <ul> <li>the <code>valid_prog_environs</code> and <code>valid_systems</code> options defined in the <code>HelloTest</code> class,</li> <li>the <code>sourcedir</code> option,</li> <li>the stage and output directory paths,</li> <li>the hash for the test, and</li> <li>the result of the test.</li> </ul>"},{"location":"testing/#part-3-configuring-tests","title":"\ud83d\udd35 Part 3: Configuring tests","text":"<p>Test configuration files set up the environment and provide options for the compilation and execution of tested programs. Let's start by investigating a simple case where a configuration file is required. Consider the test case in <code>basics/hello/hello2.py</code>.</p> <pre><code>import reframe as rfm\nimport reframe.utility.sanity as sn\n\n@rfm.simple_test\nclass HelloMultiLangTest(rfm.RegressionTest):\n    lang = parameter(['c', 'cpp'])\n\n    valid_systems = ['*']\n    valid_prog_environs = ['*']\n\n    @run_before('compile')\n    def set_sourcepath(self):\n        self.sourcepath = f'hello.{self.lang}'\n\n    @sanity_function\n    def assert_hello(self):\n        return sn.assert_found(r'Hello, World\\!', self.stdout)\n</code></pre> <p>There are 2 differences from our original \"Hello, World!\" test,</p> <ul> <li>the definition of the source files in <code>sourcepath</code> is now inside the function <code>set_sourcepath</code>, and</li> <li>there is an extra parameter <code>lang</code>.</li> </ul> <p>The sanity test remains the same. Try running the tests defined in <code>basics/hello/hello2.py</code>.</p> <pre><code>$ reframe --checkpath basics/hello/hello2.py --run\n[ReFrame Setup]\n  version:           4.3.3\n  command:           '/home/users/gkafanas/.local/easybuild/aion/2020b/epyc/software/ReFrame/4.3.3/bin/reframe --checkpath basics/hello/hello2.py --run'\n  launched by:       gkafanas@aion-0163\n  working directory: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials'\n  settings files:    '&lt;builtin&gt;'\n  check search path: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/basics/hello/hello2.py'\n  stage directory:   '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/stage'\n  output directory:  '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/output'\n  log files:         '/tmp/rfm-myh24uz1.log'\n\n[==========] Running 2 check(s)\n[==========] Started on Fri Mar  8 13:42:48 2024\n\n[----------] start processing checks\n[ RUN      ] HelloMultiLangTest %lang=cpp /b059a3fc @generic:default+builtin\n[ RUN      ] HelloMultiLangTest %lang=c /ed3216b7 @generic:default+builtin\n[     FAIL ] (1/2) HelloMultiLangTest %lang=cpp /b059a3fc @generic:default+builtin\n==&gt; test failed during 'compile': test staged in '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/stage/generic/default/builtin/HelloMultiLangTest_b059a3fc'\n[       OK ] (2/2) HelloMultiLangTest %lang=c /ed3216b7 @generic:default+builtin\n[----------] all spawned checks have finished\n\n[  FAILED  ] Ran 2/2 test case(s) from 2 check(s) (1 failure(s), 0 skipped, 0 aborted)\n[==========] Finished on Fri Mar  8 13:42:48 2024\n=============================================================================================================================================================================\nSUMMARY OF FAILURES\n-----------------------------------------------------------------------------------------------------------------------------------------------\nFAILURE INFO for HelloMultiLangTest %lang=cpp (run: 1/1)\n  * Description:\n  * System partition: generic:default\n  * Environment: builtin\n  * Stage directory: /mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/stage/generic/default/builtin/HelloMultiLangTest_b059a3fc\n  * Node list:\n  * Job type: local (id=None)\n  * Dependencies (conceptual): []\n  * Dependencies (actual): []\n  * Maintainers: []\n  * Failing phase: compile\n  * Rerun with '-n /b059a3fc -p builtin --system generic:default -r'\n  * Reason: build system error: I do not know how to compile a C++ program\n-----------------------------------------------------------------------------------------------------------------------------------------------\nLog file(s) saved in '/tmp/rfm-myh24uz1.log'\n</code></pre> <p>You can see that the tests failed. Before investigating the cause of the failure, let's explore the role of the <code>lang = parameter(['c', 'cpp'])</code> definition. The call to the <code>parameter</code> function will instantiate an object of the test class for each of the elements in its input list. And for each object, it will set the <code>lang</code> field to the element of the list for which the object is being instantiated.</p> <p>The instantiated tests are listed in the test output, <pre><code>[ RUN      ] HelloMultiLangTest %lang=cpp /b059a3fc @generic:default+builtin\n[ RUN      ] HelloMultiLangTest %lang=c /ed3216b7 @generic:default+builtin\n</code></pre> and for each test the name of the test now includes the <code>lang</code> field and its assigned value. Furthermore, if you explore the <code>output</code> or <code>staging</code> directories, you will observe that there are 2 test directories created for <code>HelloMultiLangTest</code>, one for each value of <code>lang</code>. However, the directories are differentiated by the test hash value. <pre><code>$ ls output/generic/default/builtin\nHelloMultiLangTest_b059a3fc  HelloMultiLangTest_ed3216b7\n</code></pre> The hash values are used to create the test directories instead of the actual test names because the test names become unmanageable as the number of parameters increases. Use the test output or the reports in <code>~/.reframe/reports</code> to map hashes to parameter values.</p> <p>We have seen how parameters are defined, but how is the <code>lang</code> parameter used in the <code>HelloMultiLangTest</code> test? Observe the <code>set_sourcepath</code> function. The function is decorated with the <code>@run_before('compile')</code> decorator, which as the name suggests will call the function before the compilation phase of the regression test pipeline. The function simply sets the source path of the compiled file, using the <code>lang</code> field to set the language extension. In our case this allows us to check 2 programs, one in C and another one in C++. However, it seems that the compilation of the C++ test failed.</p> <p>Question 5: Detecting cases in the report file</p> <p>Run the <code>HelloMultiLangTest</code> again and find the corresponding report file.</p> <ul> <li>How many test cases are mentioned in the report file?</li> <li>Note down in your report the <code>name</code> and <code>hash</code> fields for each test case.</li> <li>Note down in your report the value of the source path option used for each test case.</li> </ul>"},{"location":"testing/#configuring-the-environment-on-a-stand-alone-machine","title":"Configuring the environment on a stand-alone machine","text":"<p>The C++ test fails during the compilation phase of the regression test pipeline. The error message reported by ReFrame, <pre><code>==&gt; test failed during 'compile': test staged in '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/stage/generic/default/builtin/HelloMultiLangTest_b059a3fc'\n</code></pre> informs us about the phase where the test failed, and directs us to the staging directory for the test. After a failure, ReFrame retains the contents of the staging directory so that we can diagnose the cause of the problem. In this case, though the staging directory contains the source files and after some inspection, there are no obvious bugs. Let's look instead at the summary of failures printed at the end of the test.</p> <pre><code>SUMMARY OF FAILURES\n-----------------------------------------------------------------------------------------------------------------------------------------------\nFAILURE INFO for HelloMultiLangTest %lang=cpp (run: 1/1)\n  * Description:\n  * System partition: generic:default\n  * Environment: builtin\n  * Stage directory: /mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/stage/generic/default/builtin/HelloMultiLangTest_b059a3fc\n  * Node list:\n  * Job type: local (id=None)\n  * Dependencies (conceptual): []\n  * Dependencies (actual): []\n  * Maintainers: []\n  * Failing phase: compile\n  * Rerun with '-n /b059a3fc -p builtin --system generic:default -r'\n  * Reason: build system error: I do not know how to compile a C++ program\n-----------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>The ReFrame program reports that it is unable to compile C++ programs. The <code>builtin</code> environment we are using to compile our programs defines only the C compiler!</p> <p>To compile our program, we need to define an environment with a C++ compiler. The environment for an Aion cluster node is defined in <code>config/aion_node.py</code>.</p> <pre><code>site_configuration = {\n    'systems': [\n        {\n            'name': 'aion_node',\n            'descr': 'Stand alone Aion node',\n            'hostnames': [r'aion-[0-9]{4}'],\n            'modules_system': 'nomod',\n            'partitions': [\n                {\n                    'name': 'default',\n                    'scheduler': 'local',\n                    'launcher': 'local',\n                    'environs': ['gnu'],\n                }\n            ]\n        }\n    ],\n    'environments': [\n        {\n            'name': 'gnu',\n            'cc': 'gcc',\n            'cxx': 'g++',\n            'ftn': 'gfortran',\n            'target_systems': ['aion_node']\n        },\n        {\n            'name': 'clang',\n            'cc': 'clang',\n            'cxx': 'clang++',\n            'ftn': '',\n            'target_systems': ['aion_node']\n        },\n    ]\n}\n</code></pre> <p>The file defines a <code>site_configuration</code>, which consists of a list of systems and a list of environments. The systems are simply the computers in our configuration where we can build and run code, and the environments are the possible compilation setups. At this point, we still consider the nodes of our HPC cluster as stand-alone machines, so we do not specify the module system and most importantly the scheduler.</p> <p>Let's examine the components of our system configuration. First, there is a list of systems. We define a single system, <code>aion_node</code>. Important entries in the system definition are</p> <ul> <li>the <code>hostname</code> which describes for which hostnames the configuration will be active, and its value is a list of Python regular expressions (<code>re</code>),</li> <li>the <code>modules_system</code> which describes the module systems that can be used in the machine, and</li> <li>the partition of our system in which we can compile code and launch jobs.</li> </ul> <p>Since we are considering Aion nodes as standalone systems, there is no scheduler. The <code>gnu</code> compilation environment is enabled for Aion nodes, which will provide the C, C++, and Fortran compilers installed with the operating system of Aion. We also define a compilation environment for <code>clang</code>, but we do not enable it in the <code>environs</code> of the system partition, since Clang is not installed by default on our nodes.</p> <p>To run the tests with a given configuration use the <code>--config-file</code> flag with the configuration file as argument. This is the result of running our tests for the 'Hello, World!' programs. <pre><code>$ reframe --config-file config/aion_node.py --checkpath basics/hello/hello2.py --run\n[ReFrame Setup]\n  version:           4.3.3\n  command:           '/home/users/gkafanas/.local/easybuild/aion/2020b/epyc/software/ReFrame/4.3.3/bin/reframe --config-file config/aion_node.py --checkpath basics/hello/hello2.py --run'\n  launched by:       gkafanas@aion-0302\n  working directory: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials'\n  settings files:    '&lt;builtin&gt;', 'config/aion_node.py'\n  check search path: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/basics/hello/hello2.py'\n  stage directory:   '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/stage'\n  output directory:  '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/output'\n  log files:         '/tmp/rfm-1fqfvzvx.log'\n\n[==========] Running 2 check(s)\n[==========] Started on Fri Mar  8 16:15:46 2024 \n\n[----------] start processing checks\n[ RUN      ] HelloMultiLangTest %lang=cpp /b059a3fc @aion_node:default+gnu\n[ RUN      ] HelloMultiLangTest %lang=c /ed3216b7 @aion_node:default+gnu\n[       OK ] (1/2) HelloMultiLangTest %lang=c /ed3216b7 @aion_node:default+gnu\n[       OK ] (2/2) HelloMultiLangTest %lang=cpp /b059a3fc @aion_node:default+gnu\n[----------] all spawned checks have finished\n\n[  PASSED  ] Ran 2/2 test case(s) from 2 check(s) (0 failure(s), 0 skipped, 0 aborted)\n[==========] Finished on Fri Mar  8 16:15:47 2024 \nLog file(s) saved in '/tmp/rfm-1fqfvzvx.log'\n</code></pre> Both tests now finish successfully. The system configuration used for the test is now reported as <code>@aion_node:default+gnu</code>.</p> <p>Up to this point, we were running the tests defined in a single file. ReFrame can run tests from multiple files and offers a number of options to customize our runs. For instance, to run all the tests in <code>basic/hello</code>, you can provide this path to the <code>--checkpath</code> option and use the <code>--recursive</code> option. <pre><code>$ reframe --config-file config/aion_node.py --checkpath basics/hello --recursive --run\n[ReFrame Setup]\n  version:           4.3.3\n  command:           '/home/users/gkafanas/.local/easybuild/aion/2020b/epyc/software/ReFrame/4.3.3/bin/reframe --config-file config/aion_node.py --checkpath basics/hello --recursive --run'\n  launched by:       gkafanas@aion-0073\n  working directory: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials'\n  settings files:    '&lt;builtin&gt;', 'config/aion_node.py'\n  check search path: (R) '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/basics/hello'\n  stage directory:   '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/stage'\n  output directory:  '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/output'\n  log files:         '/tmp/rfm-889p5pos.log'\n\n[==========] Running 3 check(s)\n[==========] Started on Fri Mar  8 18:14:15 2024 \n\n[----------] start processing checks\n[ RUN      ] HelloTest /2b3e4546 @aion_node:default+gnu\n[ RUN      ] HelloMultiLangTest %lang=cpp /b059a3fc @aion_node:default+gnu\n[ RUN      ] HelloMultiLangTest %lang=c /ed3216b7 @aion_node:default+gnu\n[       OK ] (1/3) HelloTest /2b3e4546 @aion_node:default+gnu\n[       OK ] (2/3) HelloMultiLangTest %lang=c /ed3216b7 @aion_node:default+gnu\n[       OK ] (3/3) HelloMultiLangTest %lang=cpp /b059a3fc @aion_node:default+gnu\n[----------] all spawned checks have finished\n\n[  PASSED  ] Ran 3/3 test case(s) from 3 check(s) (0 failure(s), 0 skipped, 0 aborted)\n[==========] Finished on Fri Mar  8 18:14:17 2024 \nLog file(s) saved in '/tmp/rfm-889p5pos.log'\n</code></pre></p> <p>You can also filter the tests you want to run by name using the <code>--name</code> option and a Python regular expression to filter your tests. For instance, to run only the <code>HelloMultiLangTest</code> test use the following call.</p> <pre><code>$ reframe --config-file config/aion_node.py --checkpath basics/hello --recursive --name 'HelloMultiLangTest' --run\n[ReFrame Setup]\n  version:           4.3.3\n  command:           '/home/users/gkafanas/.local/easybuild/aion/2020b/epyc/software/ReFrame/4.3.3/bin/reframe --config-file config/aion_node.py --checkpath basics/hello --recursive --name HelloMultiLangTest --run'\n  launched by:       gkafanas@aion-0073\n  working directory: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials'\n  settings files:    '&lt;builtin&gt;', 'config/aion_node.py'\n  check search path: (R) '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/basics/hello'\n  stage directory:   '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/stage'\n  output directory:  '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/output'\n  log files:         '/tmp/rfm-ant49hjx.log'\n\n[==========] Running 2 check(s)\n[==========] Started on Fri Mar  8 18:33:03 2024 \n\n[----------] start processing checks\n[ RUN      ] HelloMultiLangTest %lang=cpp /b059a3fc @aion_node:default+gnu\n[ RUN      ] HelloMultiLangTest %lang=c /ed3216b7 @aion_node:default+gnu\n[       OK ] (1/2) HelloMultiLangTest %lang=c /ed3216b7 @aion_node:default+gnu\n[       OK ] (2/2) HelloMultiLangTest %lang=cpp /b059a3fc @aion_node:default+gnu\n[----------] all spawned checks have finished\n\n[  PASSED  ] Ran 2/2 test case(s) from 2 check(s) (0 failure(s), 0 skipped, 0 aborted)\n[==========] Finished on Fri Mar  8 18:33:04 2024 \nLog file(s) saved in '/tmp/rfm-ant49hjx.log'\n</code></pre> <p>Question 6: Composing configuration files</p> <p>The <code>aion_node.py</code> configuration file enables us to run jobs only on a single compute node of Aion. Let's extend it to support a compute node from Aion or Iris.</p> <ul> <li>Create a new configuration file <code>ulhpc_node.py</code> as a copy of <code>aion_node.py</code>.</li> <li>Create a new system entry for Iris nodes. Note down the new system configuration in your report.</li> <li>Replace the pattern for Aion node hostnames (<code>r'aion-[0-9]{4}'</code>) with another regular expression. The regular expression does not have to be equivalent to the existing one, just to capture the hostnames of Aion nodes. Note down the new pattern in your report.</li> <li>Add the new <code>ulhpc_node.py</code> to your GitHub Classroom repository.</li> </ul> <p>Note: To find the right matching pattern, check the documentation for Python regular expressions or the Python Regex Cheat Sheet.</p>"},{"location":"testing/#configuring-the-environment-on-a-cluster","title":"Configuring the environment on a cluster","text":"<p>We have run tests for a local computer. Now, we would like to ReFrame to run the tests on the HPC cluster, taking advantage of SLURM and the modules. Have a look at the new configuration in <code>config/aion.py</code>.</p> <pre><code>site_configuration = {\n    'systems': [\n        {\n            'name': 'aion',\n            'descr': 'Aion cluster',\n            'hostnames': [r'aion-[0-9]{4}'],\n            'modules_system': 'lmod',\n            'partitions': [\n                {\n                    'name': 'batch',\n                    'descr': 'Aion compute nodes',\n                    'scheduler': 'slurm',\n                    'launcher': 'srun',\n                    'access': ['--partition=batch', '--qos=normal'],\n                    'max_jobs':  8,\n                    'environs': ['gnu', 'builtin'],\n                }\n            ]\n        }\n    ],\n    'environments': [\n        {\n            'name': 'gnu',\n            'modules': ['toolchain/foss/2020b'],\n            'cc': 'gcc',\n            'cxx': 'g++',\n            'ftn': 'gfortran',\n            'target_systems': ['aion']\n        },\n        {\n            'name': 'builtin',\n            'cc': 'gcc',\n            'cxx': 'g++',\n            'ftn': 'gfortran',\n            'target_systems': ['aion']\n        },\n    ]\n}\n</code></pre> <p>This configuration file allows ReFrame to submit jobs on the cluster to execute the tests. The important components of the specifications are</p> <ul> <li>the module system, <code>lmod</code>, is defined for our configuration and corresponds to the module system supported in Aion, and</li> <li>the compilation environment <code>gnu</code>, which now uses the <code>toolchain/foss/2020b</code> module to compile our source code.</li> </ul> <p>We have also defined a <code>builtin</code> compilation environment that uses the compiler of the operating system, but <code>toolchain/foss/2020b</code> provides more compilers and other packages and has more up-to-date compiler versions installed.</p> <p>Our example uses the test definitions in the <code>basics/hellomp</code>. The first definition is <code>basics/hellomp/hellomp1.py</code>. <pre><code>import reframe as rfm\nimport reframe.utility.sanity as sn\n\n@rfm.simple_test\nclass HelloThreadedTest(rfm.RegressionTest):\n    valid_systems = ['*']\n    valid_prog_environs = ['*']\n    sourcepath = 'hello_threads.cpp'\n    build_system = 'SingleSource'\n    executable_opts = ['16']\n\n    @run_before('compile')\n    def set_compilation_flags(self):\n        self.build_system.cxxflags = ['-std=c++11', '-Wall']\n        environ = self.current_environ.name\n        if environ in {'builtin', 'gnu'}:\n            self.build_system.cxxflags += ['-pthread']\n\n    @run_before('run')\n    def set_computational_resources(self):\n        system = self.current_system.name\n        if system in {'aion'}:\n            self.job.options = ['--time=00:05:00', '--nodes=1', '--ntasks-per-node=1', '--cpus-per-task=16']\n\n    @sanity_function\n    def assert_hello(self):\n        return sn.assert_found(r'Hello, World\\!', self.stdout)\n</code></pre> This is a test for a multithreaded version of the 'Hello, World!' program, where each thread prints a greeting. It will be tested on all compilation environments in our configuration file.</p> <ul> <li>For the compilation environments <code>builtin</code> and <code>gnu</code> the test sets the <code>-pthread</code> compilation flag.</li> <li>For the execution in the cluster the test explicitly sets the flags <code>--time</code>, <code>--nodes</code>, <code>--ntasks-per-node</code>, and <code>--cpus-per-task</code>. The flags <code>--partition</code> and <code>--qos</code> are inherited from the system configuration and are set implicitly.</li> </ul> <p>Question 7: Composing configuration files for a cluster</p> <p>The configuration file <code>aion.py</code> targets the Aion cluster. Create a configuration file <code>ulhpc.py</code> that targets both Aion and Iris systems. Add the configuration file to your GitHub Classroom repository.</p>"},{"location":"testing/#executing-the-tests","title":"Executing the tests","text":"<p>Execute the tests as usual. <pre><code>$ reframe --config-file config/aion.py --checkpath basics/hellomp/hellomp1.py --run\n[ReFrame Setup]\n  version:           4.3.3\n  command:           '/home/users/gkafanas/.local/easybuild/aion/2020b/epyc/software/ReFrame/4.3.3/bin/reframe --config-file config/aion.py --checkpath basics/hellomp/hellomp1.py --run' \n  launched by:       gkafanas@aion-0034\n  working directory: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials'\n  settings files:    '&lt;builtin&gt;', 'config/aion.py'\n  check search path: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/basics/hellomp/hellomp1.py'\n  stage directory:   '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/stage'\n  output directory:  '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/output'\n  log files:         '/tmp/rfm-o6oz0qqw.log'\n\n[==========] Running 1 check(s)\n[==========] Started on Fri Mar  8 21:57:34 2024\n\n[----------] start processing checks\n[ RUN      ] HelloThreadedTest /a6fa300f @aion:batch+gnu\n[ RUN      ] HelloThreadedTest /a6fa300f @aion:batch+builtin\n[       OK ] (1/2) HelloThreadedTest /a6fa300f @aion:batch+builtin\n[       OK ] (2/2) HelloThreadedTest /a6fa300f @aion:batch+gnu\n[----------] all spawned checks have finished\n\n[  PASSED  ] Ran 2/2 test case(s) from 1 check(s) (0 failure(s), 0 skipped, 0 aborted)\n[==========] Finished on Fri Mar  8 21:57:39 2024\nLog file(s) saved in '/tmp/rfm-o6oz0qqw.log'\n</code></pre></p> <p>We can see that 2 tests were executed, one for each compilation environment. Let's investigate the resulting scripts for each phase.</p>"},{"location":"testing/#the-compilation-script","title":"The compilation script","text":"<p>You can find the compilation script in the <code>output</code> directory as usual. Open the compilation script <code>output/aion/batch/gnu/HelloThreadedTest/rfm_build.sh</code> for the <code>gnu</code> environment. <pre><code>#!/bin/bash\n\n_onerror()\n{\n    exitcode=$?\n    echo \"-reframe: command \\`$BASH_COMMAND' failed (exit code: $exitcode)\"\n    exit $exitcode\n}\n\ntrap _onerror ERR\n\nmodule load toolchain/foss/2020b\ng++ -std=c++11 -Wall -pthread hello_threads.cpp -o ./HelloThreadedTest\n</code></pre> We observe that module <code>toolchain/foss/2020b</code> is loaded in the environment before the compilation, but the compilation was executed locally. Next, have a look at <code>output/aion/batch/builtin/HelloThreadedTest/rfm_build.sh</code>, the compilation script for the <code>builtin</code> environment. <pre><code>#!/bin/bash\n\n_onerror()\n{\n    exitcode=$?\n    echo \"-reframe: command \\`$BASH_COMMAND' failed (exit code: $exitcode)\"\n    exit $exitcode\n}\n\ntrap _onerror ERR\n\ng++ -std=c++11 -Wall -pthread hello_threads.cpp -o ./HelloThreadedTest\n</code></pre> The only difference between <code>gnu</code> and <code>builtin</code> is that the toolchain module is not loaded and thus the system compiler is used instead.</p>"},{"location":"testing/#the-execution-script","title":"The execution script","text":"<p>Open the execution script, <code>output/aion/batch/gnu/HelloThreadedTest/rfm_job.sh</code>, corresponding to the <code>gnu</code> compilation environment. <pre><code>#!/bin/bash\n#SBATCH --job-name=\"rfm_HelloThreadedTest\"\n#SBATCH --ntasks=1\n#SBATCH --output=rfm_job.out\n#SBATCH --error=rfm_job.err\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --time=00:05:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=16\nmodule load toolchain/foss/2020b\nsrun ./HelloThreadedTest 16\n</code></pre></p> <p>This is a simple batch script that executes our compiled program on 16 cores, corresponding to a full socket of an Aion compute node. Note that the modules used in the compilation are loaded automatically. Now browse the execution script <code>output/aion/batch/builtin/HelloThreadedTest/rfm_job.sh</code> for the <code>builtin</code> environment. <pre><code>#!/bin/bash\n#SBATCH --job-name=\"rfm_HelloThreadedTest\"\n#SBATCH --ntasks=1\n#SBATCH --output=rfm_job.out\n#SBATCH --error=rfm_job.err\n#SBATCH --partition=batch\n#SBATCH --qos=normal\n#SBATCH --time=00:05:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=16\nsrun ./HelloThreadedTest 16\n</code></pre> The execution command for the <code>builtin</code> and <code>gnu</code> compilation environments is identical, except that there is no module loaded in the <code>builtin</code> environment.</p> <p>In general, ReFrame generates the job shell scripts using the following pattern: <pre><code>#!/bin/bash -l\n{job_scheduler_preamble}\n{prepare_cmds}\n{env_load_cmds}\n{prerun_cmds}\n{parallel_launcher} {executable} {executable_opts}\n{postrun_cmds}\n</code></pre> You can find more configuration options for the job script in the official tutorial.</p> <p>Compiling test executables in a cluster</p> <p>Note that the test executable is compiled in the node where ReFrame is launched. After the compilation, multiple tests can then be launched in parallel if a scheduler is available. ReFrame is designed to handle multiple sanity and performance tests for each executable that require significantly more time than the compilation of the executable. You can find more details in the documentation for the test pipelines.</p> <p>To compile and test multiple executables in parallel, we need to launch multiple instances of ReFrame. If a scheduler is available multiple runs of ReFrame can be scheduled as independent jobs.</p> <p>Question 8: Composing tests for a cluster</p> <p>The multithreaded \"Hello, World!\" test was designed to run with 16 threads. Modify the test in <code>basics/hellomp/hellomp1.py</code> to run with 8 and 32 threads. You should use the parametric syntax for ReFrame as explained earlier in the example <code>basics/hello/hello2.py</code>.</p> <p>Submit the modified test file in your Github Classroom repository.</p>"},{"location":"testing/#part-4-advanced-sanity-checks","title":"\ud83d\udd35 Part 4: Advanced sanity checks","text":"<p>Let's observe the output generated for our test in the <code>gnu</code> compilation environment stored in the <code>output/aion/batch/gnu/HelloThreadedTest/rfm_job.out</code> file. <pre><code>[[ 0] Hello, World!\n [ 12] Hello, World!\n] Hello, World!\n[ 3] Hello, World!\n[ 4] Hello, World!\n[ 5] Hello, World!\n[ 6] Hello, World!\n[ 7] Hello, World!\n[ 8] Hello, World!\n[ 9] Hello, World!\n[10] Hello, World!\n[11] Hello, World!\n[12] Hello, World!\n[13] Hello, World!\n[14] Hello, World!\n[15] Hello, World!\n</code></pre> This is not the intended output, but our test still succeeds! As mentioned previously, our sanity check simply checks for the occurrence of the phrase 'Hello, World!' in the output. <pre><code>@sanity_function\ndef assert_hello(self):\n    return sn.assert_found(r'Hello, World\\!', self.stdout)\n</code></pre></p> <p>To rectify our regression test we use the following sanity check, which looks for exactly 16 occurrences of the greeting. <pre><code>@sanity_function\ndef assert_num_messages(self):\n    num_messages = sn.len(sn.findall(r'\\[\\s?\\d+\\] Hello, World\\!',\n                                     self.stdout))\n    return sn.assert_eq(num_messages, 16)\n</code></pre></p> <p>The code for the new sanity check is found in <code>basics/hellomp/hellomp2.py</code>. Running the test with the command: <pre><code>reframe --config-file config/aion.py --checkpath basics/hellomp/hellomp2.py --run\n</code></pre> You should get an error reporting the mismatched output.</p> <p>Tests failing or not failing? This is a Race Condition bug!</p> <p>If you repeat the test multiple times, you can notice that sometimes the tests pass, and sometimes they fail. This is a typical example of a race condition that is a time-dependent error (cf our Lecture on Debugging). Automatic testing testing that is repeated many times is usually crucial to detect this type of errors.</p> <p>For this specific program, we have different threads that are printing an output message all at the same time. Depending on how the threads are scheduled on the computing cores by the operating system, the message might be interleaved or not.</p> <p>Question 9: Repeating sanity checks to catch non-deterministic errors</p> <p>Reframe offers 2 options for repeating a test multiple times in order to catch non-deterministic errors, <code>--repeat</code> and <code>--rerun</code>.</p> <ul> <li>With the <code>--repeat</code>, each test is repeated a given number of times.</li> <li>With the <code>--rerun</code>, the whole test session is rerun a given number of times.</li> </ul> <p>Your task:</p> <ul> <li> <p>Edit <code>basics/hellomp/hellomp2.py</code> to run 2 parallel threads to make the race condition harder to detect. Then use either <code>--rerun</code> or <code>--repeat</code> to run the test 8 times. Were all tests successful? Were you able to catch the race condition?</p> </li> <li> <p>Add the ReFrame report files for your run from <code>~/.reframe/reports</code> to your GitHub Classroom repository.</p> </li> </ul> <p>Note: You should be able to detect the race condition. But don't worry if you don't, because it is a random process.</p> <p>This race condition problem in our code can be rectified by activating the use of the mutex with the flag <code>SYNC_MESSAGES</code>. We tell ReFrame to inject this flag in the compilation command line by adding the following line to the test specification (cf new file <code>basics/hellomp/hellomp3.py</code>). <pre><code>self.build_system.cppflags = ['-DSYNC_MESSAGES']\n</code></pre> Run the new test to ensure that the problem is fixed.</p>"},{"location":"testing/#part-5-performance-testing","title":"\ud83d\udd35 Part 5: Performance testing","text":"<p>Up to this point, we have tried sanity checks, however, ReFrame also allows us to define performance regression checks. During development, programmers are usually focused on correctness. As a result, while most correctness errors are detected and fixed relatively early, subtle degradation in performance as more functionality is added is usually hard to detect. Regularly testing a code base with ReFrame performance tests is a great way to track the performance of your code base.</p> <p>We will test the performance of a simple program writing and reading from a stream. The test files are located in the directory <code>basics/stream</code>. We implement the same set of tests in an increasingly more sophisticated manner. Open the file <code>basics/stream/stream1.py</code> first. There is a single test called <code>StreamTest</code> defined, with 4 performance measurements.</p> <ul> <li>Performance measurement tests inherit from the <code>reframe.RegressionTest</code> class which defines a dictionary <code>perf_variables</code> that maps test names to test definitions. The tests are actually classes, and after the execution of the test, they also store the test results.</li> <li>Performance measures are defined in functions decorated with the <code>@performance_function</code> decorator which takes as inputs the units of measurement and other parameters that are optional. If you provide the <code>perf_key</code> parameter, its value will be used as the key for the test in the <code>perf_variables</code> dictionary, otherwise, the function name is used by default.</li> </ul> <p>Consider for instance the test measuring the copy speed: <pre><code>@performance_function('MB/s', perf_key='Copy')\ndef extract_copy_perf(self):\n    return sn.extractsingle(r'Copy:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n</code></pre> A unit of measurement must be provided as the first argument to the generator, in this case, <code>'MB/s'</code>. The actual value for the test measurement is extracted from the output of the tested executable. The performance measurement function uses the deferrable function <code>extractsingle</code> which extracts a single value from a line containing the given pattern and converts it to a <code>float</code> according to argument 4.</p>"},{"location":"testing/#executing-performance-tests","title":"Executing performance tests","text":"<p>Performance tests are run similarly to sanity checks. In the case of the stream test for instance: <pre><code>$ reframe --config-file config/aion.py --checkpath basics/stream/stream1.py --run\n[ReFrame Setup]\n  version:           4.3.3\n  command:           '/home/users/gkafanas/.local/easybuild/aion/2020b/epyc/software/ReFrame/4.3.3/bin/reframe --config-file config/aion.py --checkpath basics/stream/stream1.py --run'\n  launched by:       gkafanas@aion-0019\n  working directory: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials'\n  settings files:    '&lt;builtin&gt;', 'config/aion.py'\n  check search path: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/basics/stream/stream1.py'\n  stage directory:   '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/stage'\n  output directory:  '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/reframe/tutorials/output'\n  log files:         '/tmp/rfm-yn3itbkl.log'\n\n[==========] Running 1 check(s)\n[==========] Started on Fri Mar  8 23:29:55 2024\n\n[----------] start processing checks\n[ RUN      ] StreamTest /cdf4820d @aion:batch+gnu\n[       OK ] (1/1) StreamTest /cdf4820d @aion:batch+gnu\nP: Copy: 22771.9 MB/s (r:0, l:None, u:None)\nP: Scale: 22880.7 MB/s (r:0, l:None, u:None)\nP: Add: 25043.7 MB/s (r:0, l:None, u:None)\nP: Triad: 25085.8 MB/s (r:0, l:None, u:None)\n[----------] all spawned checks have finished\n\n[  PASSED  ] Ran 1/1 test case(s) from 1 check(s) (0 failure(s), 0 skipped, 0 aborted)\n[==========] Finished on Fri Mar  8 23:30:03 2024\nLog file(s) saved in '/tmp/rfm-yn3itbkl.log'\n</code></pre> After each test, the performance results are also reported: <pre><code>P: Copy: 22771.9 MB/s (r:0, l:None, u:None)\nP: Scale: 22880.7 MB/s (r:0, l:None, u:None)\nP: Add: 25043.7 MB/s (r:0, l:None, u:None)\nP: Triad: 25085.8 MB/s (r:0, l:None, u:None)\n</code></pre> You can add the parameter flag <code>--performance-report</code> at the call to <code>reframe</code> to print a detailed report of the performance analysis at the end of tests.</p>"},{"location":"testing/#adding-performance-reference-values","title":"Adding performance reference values","text":"<p>The ReFrame performance test definitions are enough to extract performance measurements, but without any context, we can not interpret the resulting values. Typically in regression testing, we would like our tests to report when the performance of our program deviates significantly from some expected values. Reference values for our tests are defined using the <code>reference</code> field: <pre><code>reference = {\n    'aion': {\n        'Copy':  (33000, -0.05, 0.05, 'MB/s'),\n        'Scale': (15500, -0.05, 0.05, 'MB/s'),\n        'Add':   (16000, -0.05, 0.05, 'MB/s'),\n        'Triad': (16000, -0.05, 0.05, 'MB/s')\n    }\n}\n</code></pre> The reference field is a dictionary that defines for each system configuration the expected values for each performance measurement, and the acceptable deviation on either side. Use <code>None</code> if you would like to avoid checking whether the performance exceeds either of the limits.</p> <p>Question 10: Using performance tests with references</p> <p>You will find in the tutorial directory the file <code>basics/stream/stream2.py</code>. The test file contains reference performance values for the single-threaded stream benchmark on Aion.</p> <ul> <li> <p>Convert the benchmark to multithreaded by replacing the class variable definition, <pre><code>env_vars = {\n    'OMP_NUM_THREADS': '1',\n    'OMP_PLACES': 'cores'\n}\n</code></pre> with: <pre><code>env_vars = {\n    'OMP_NUM_THREADS': '16',\n    'OMP_PLACES': 'cores'\n}\n</code></pre></p> </li> <li> <p>Run the test again, provide a screenshot of the ReFrame output, and explain based on the output whether the test failed or succeeded.</p> </li> <li> <p>Update the reference values for the <code>aion</code> configuration, to ensure that the reference corresponds to a multithreaded test. Now your test should succeed. Provide a screenshot of the ReFrame output in your report.</p> </li> </ul>"},{"location":"testing/#part-6-integration-with-build-systems","title":"\ud83d\udd35 Part 6: Integration with build systems","text":"<p>Up to this point, we encountered single source programs that ReFrame compiles with a few direct compilation commands. Programs that involve multiple source files or elaborate linking dependencies, cannot be handled so easily with ReFrame. Luckily, ReFrame provides options to call various build automation systems. The build systems are categorized broadly by ReFrame in</p> <ul> <li>build automation systems, such as Make, CMake, and Autotools, and</li> <li>source code-based package managers such as EasyBuild and Spack.</li> </ul> <p>The main difference between the 2 categories is that source code base package managers automate the installation of all dependencies, whereas build automation systems require manual handling of dependencies which can be laborious when testing a piece of software in a large number of platforms.</p>"},{"location":"testing/#compiling-the-source-code-with-makefile-scripts","title":"Compiling the source code with Makefile scripts","text":"<p>You can find an example test of a program compiled with Makefiles in <code>advanced/makefile</code>. The tests are defined in the file <code>advanced/makefiles/maketest.py</code>.</p> Test definitions in <code>advanced/makefiles/maketest.py</code> <pre><code>import reframe as rfm\nimport reframe.utility.sanity as sn\n\n@rfm.simple_test\nclass MakefileTest(rfm.RegressionTest):\n    elem_type = parameter(['float', 'double'])\n\n    descr = 'Test demonstrating use of Makefiles'\n    valid_systems = ['*']\n    valid_prog_environs = ['builtin', 'gnu']\n    executable = './dotprod'\n    executable_opts = ['100000']\n    build_system = 'Make'\n\n    @run_before('compile')\n    def set_compiler_flags(self):\n        self.build_system.cppflags = [f'-DELEM_TYPE={self.elem_type}']\n\n    @sanity_function\n    def validate_test(self):\n        return sn.assert_found(rf'Result \\({self.elem_type}\\):', self.stdout)\n\n@rfm.simple_test\nclass MakeOnlyTest(rfm.CompileOnlyRegressionTest):\n    elem_type = parameter(['float', 'double'])\n    descr = 'Test demonstrating use of Makefiles'\n    valid_systems = ['*']\n    valid_prog_environs = ['builtin', 'gnu']\n    build_system = 'Make'\n\n    @run_before('compile')\n    def set_compiler_flags(self):\n        self.build_system.cppflags = [f'-DELEM_TYPE={self.elem_type}']\n\n    @sanity_function\n    def validate_compilation(self):\n        return sn.assert_not_found(r'warning', self.stdout)\n</code></pre> <ul> <li>The parameter <code>build_system</code> is used to select the 'Make' build option.</li> <li>With the <code>build_system</code> set to 'Make', the <code>src/Makefile</code> will be used to compile the executable.</li> </ul> <p>Execute the tests:</p> <pre><code>$ reframe --config-file config/aion.py --checkpath advanced/makefiles/maketest.py --run\n[ReFrame Setup]\n  version:           4.3.3\n  command:           '/home/users/gkafanas/.local/easybuild/aion/2020b/epyc/software/ReFrame/4.3.3/bin/reframe --config-file config/aion.py --checkpath advanced/makefiles/maketest.py --run'\n  launched by:       gkafanas@aion-0336\n  working directory: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/W05-Testing_on_HPC/tutorials'\n  settings files:    '&lt;builtin&gt;', 'config/aion.py'\n  check search path: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/W05-Testing_on_HPC/tutorials/advanced/makefiles/maketest.py'\n  stage directory:   '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/W05-Testing_on_HPC/tutorials/stage'\n  output directory:  '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/W05-Testing_on_HPC/tutorials/output'\n  log files:         '/tmp/rfm-hcpmjmq2.log'\n\n[==========] Running 4 check(s)\n[==========] Started on Fri Mar 15 19:30:54 2024\n\n[----------] start processing checks\n[ RUN      ] MakeOnlyTest %elem_type=double /1f34b084 @aion:batch+gnu\n[ RUN      ] MakeOnlyTest %elem_type=float /dde8bef0 @aion:batch+gnu\n[ RUN      ] MakefileTest %elem_type=double /1fe42bc0 @aion:batch+gnu\n[ RUN      ] MakefileTest %elem_type=float /56e65399 @aion:batch+gnu\n[       OK ] (1/4) MakeOnlyTest %elem_type=double /1f34b084 @aion:batch+gnu\n[       OK ] (2/4) MakeOnlyTest %elem_type=float /dde8bef0 @aion:batch+gnu\n[       OK ] (3/4) MakefileTest %elem_type=double /1fe42bc0 @aion:batch+gnu\n[       OK ] (4/4) MakefileTest %elem_type=float /56e65399 @aion:batch+gnu\n[----------] all spawned checks have finished\n\n[  PASSED  ] Ran 4/4 test case(s) from 4 check(s) (0 failure(s), 0 skipped, 0 aborted)\n[==========] Finished on Fri Mar 15 19:31:10 2024\nLog file(s) saved in '/tmp/rfm-hcpmjmq2.log'\n</code></pre> <p>The important files in this case are the compilation files. Open the compilation file <code>output/aion/batch/gnu/MakefileTest_1fe42bc0/rfm_build.sh</code> for the <code>gnu</code> environment.</p> Contents of <code>output/aion/batch/gnu/MakefileTest_1fe42bc0/rfm_build.sh</code> compilation script <pre><code>#!/bin/bash\n\n_onerror()\n{\n    exitcode=$?\n    echo \"-reframe: command \\`$BASH_COMMAND' failed (exit code: $exitcode)\"\n    exit $exitcode\n}\n\ntrap _onerror ERR\n\nmodule load toolchain/foss/2020b\nmake -j 1 CC=\"gcc\" CXX=\"g++\" FC=\"gfortran\" NVCC=\"nvcc\" CPPFLAGS=\"-DELEM_TYPE=double\"\n</code></pre> <p>We observe that</p> <ul> <li><code>make</code> was used to compile our executable,</li> <li>compiler options and other flags, such as <code>ELEM_TYPE</code> are set by ReFrame in Makefile variables, and</li> <li>any dependencies must be available in the environment when compiling the program.</li> </ul> <p>Compile only regression tests</p> <p>This example demonstrates a new type of test, the <code>CompileOnlyRegressionTest</code>. These tests examine only the compilation output, they do not launch the resulting executable. In our case, the test <code>validate_compilation</code> simply ensures that no warnings were generated during the compilation.</p> <p>Compile-only tests are very useful in reporting errors when analyzing the code with static code analyzers.</p>"},{"location":"testing/#compiling-with-source-code-package-manager-systems","title":"Compiling with source code package manager systems","text":"<p>Build automation scripts can build any piece of code, but they assume that all the required dependencies will be available at build and run time. When dependencies are not available as system packages or modules, we can use source code package managers, that build and install all required dependencies locally as modules before building our software.</p> <p>An example using EasyBuild is located in <code>advanced/easybuild</code>. Our test directory contains only the file with the test definitions.</p> Contents of <code>advanced/easybuild/eb_test.py</code> <pre><code>import reframe as rfm\nimport reframe.utility.sanity as sn\n\n@rfm.simple_test\nclass BZip2EBCheck(rfm.RegressionTest):\n    descr = 'Demo test using EasyBuild to build the test code'\n    valid_systems = ['*']\n    valid_prog_environs = ['builtin']\n    executable = 'bzip2'\n    executable_opts = ['--help']\n    build_system = 'EasyBuild'\n\n    @run_before('compile')\n    def setup_build_system(self):\n        self.build_system.easyconfigs = ['bzip2-1.0.6.eb']\n        self.build_system.options = ['-f']\n\n    @run_before('run')\n    def prepare_run(self):\n        self.modules = self.build_system.generated_modules\n\n    @sanity_function\n    def assert_version(self):\n        return sn.assert_found(r'Version 1.0.6', self.stderr)\n</code></pre> <ul> <li>The parameter <code>build_system</code> is used to select the 'EasyBuild' build option.</li> <li>Instead of a directory with source code, a list of easyconfig files is specified with the <code>build_system.easyconfigs</code> field.</li> </ul> <p>Ensure that EasyBuild is accessible in your path, and run the test: <pre><code>$ reframe --config-file config/aion.py --checkpath advanced/easybuild/eb_test.py --run\n[ReFrame Setup]\n  version:           4.3.3\n  command:           '/home/users/gkafanas/.local/easybuild/aion/2020b/epyc/software/ReFrame/4.3.3/bin/reframe --config-file config/aion.py --checkpath advanced/easybuild/eb_test.py --run'\n  launched by:       gkafanas@aion-0336\n  working directory: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/W05-Testing_on_HPC/tutorials'\n  settings files:    '&lt;builtin&gt;', 'config/aion.py'\n  check search path: '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/W05-Testing_on_HPC/tutorials/advanced/easybuild/eb_test.py'\n  stage directory:   '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/W05-Testing_on_HPC/tutorials/stage'\n  output directory:  '/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/W05-Testing_on_HPC/tutorials/output'\n  log files:         '/tmp/rfm-raoymwru.log'\n\n[==========] Running 1 check(s)\n[==========] Started on Fri Mar 15 19:36:14 2024\n\n[----------] start processing checks\n[ RUN      ] BZip2EBCheck /f4d96bd6 @aion:batch+builtin\n[       OK ] (1/1) BZip2EBCheck /f4d96bd6 @aion:batch+builtin\n[----------] all spawned checks have finished\n\n[  PASSED  ] Ran 1/1 test case(s) from 1 check(s) (0 failure(s), 0 skipped, 0 aborted)\n[==========] Finished on Fri Mar 15 19:36:25 2024\nLog file(s) saved in '/tmp/rfm-raoymwru.log'\n</code></pre></p> <p>The tests were executed successfully. Observing the contents of the compilation file <code>output/aion/batch/builtin/BZip2EBCheck/rfm_build.sh</code>, <pre><code>#!/bin/bash\n\n_onerror()\n{\n    exitcode=$?\n    echo \"-reframe: command \\`$BASH_COMMAND' failed (exit code: $exitcode)\"\n    exit $exitcode\n}\n\ntrap _onerror ERR\n\nexport EASYBUILD_BUILDPATH=/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/W05-Testing_on_HPC/tutorials/stage/aion/batch/builtin/BZip2EBCheck/easybuild/build\nexport EASYBUILD_INSTALLPATH=/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/W05-Testing_on_HPC/tutorials/stage/aion/batch/builtin/BZip2EBCheck/easybuild\nexport EASYBUILD_PREFIX=/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/W05-Testing_on_HPC/tutorials/stage/aion/batch/builtin/BZip2EBCheck/easybuild\nexport EASYBUILD_SOURCEPATH=/mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/W05-Testing_on_HPC/tutorials/stage/aion/batch/builtin/BZip2EBCheck/easybuild\neb bzip2-1.0.6.eb -f\n</code></pre> we can see that a module was compiled with EasyBuild and stored in the staging directory. During test execution, the compiled module was loaded to provide the test executable: <pre><code>#!/bin/bash\n#SBATCH --job-name=\"rfm_BZip2EBCheck\"\n#SBATCH --ntasks=1\n#SBATCH --output=rfm_job.out\n#SBATCH --error=rfm_job.err\n#SBATCH --partition=batch\n#SBATCH --qos=normal\nmodule use /mnt/irisgpfs/users/gkafanas/tutorials/ReFrame/W05-Testing_on_HPC/tutorials/stage/aion/batch/builtin/BZip2EBCheck/easybuild/modules/all\nmodule load tools/bzip2/1.0.6\nsrun bzip2 --help\n</code></pre> Any dependencies of our test program, <code>bzip2</code>, were compiled and installed as modules, and are thus available when loading the <code>bzip2</code> module.</p>"},{"location":"testing/#report-submission","title":"\ud83d\udd35 Report submission","text":"<p>Don't forget to add and push all the required files to your GitHub Classroom repository:</p> <ul> <li>the log files and the configuration files ;</li> <li>the report, including the answers to all the questions and the screenshots.</li> </ul> <p>Don't add any other files and directories (for example, build directories, executables, Makefile, etc.). Finalize your report and save it as a PDF (no other format allowed!) in the <code>report</code> sub-directory of your GitHub Classroom repository.</p> Uploading your report <p>You don't need to copy it to the HPC to upload your report. Instead, you can use the Add file button directly on the GitHub webpage.</p> <p>Files Submission</p> <p>Your files and report are considered submitted once they are pushed to your repository.</p> <p>Check your GitHub repository online to make sure: https://github.com/MHPC-HPCSoftwareEnvironment/w05-testing-on-hpc-XXX</p> <p>Passed the deadline at 25/03/2024 23h59 you won't be able to push anymore.</p>"},{"location":"testing/#additional-resources","title":"\ud83d\udd35 Additional Resources","text":""},{"location":"testing/#about-reframe","title":"About ReFrame","text":"<ul> <li>ReFrame Documentation</li> <li>ReFrame Command Line Reference</li> <li>ReFrame Tutorial</li> </ul>"},{"location":"testing/#about-python-regular-expressions","title":"About Python Regular Expressions","text":"<ul> <li>Python Documentation on <code>re</code>: Regular expression operations</li> <li>Regex Cheat Sheet \u2013 Python</li> <li>Regular Expressions Cheat Sheet</li> <li>regex101: build, test, and debug regex</li> </ul>"}]}